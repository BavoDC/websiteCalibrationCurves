[{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"risk-prediction-models","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models","what":"Risk prediction models","title":"Introduction to the CalibrationCurves package","text":"package, focus risk prediction models estimate probability \\(\\pi_i\\) observing event. use \\(y_i \\(0, 1)\\) denote variable captures outcome takes value 0 case non-event 1 case event. , \\(\\) serves index observations (mostly patient within medical predictive analytics) \\(= (1, \\dots, n)\\) \\(n\\) denotes total number observations. assume response variable \\(y_i\\) follows Bernoulli distribution \\(y_i \\sim \\text{Bern}(\\pi_i)\\). example, interested estimating probability \\(\\pi_i\\) observing malignant tumour patient \\(\\). case, event \\(y_i = 1\\) tumour malignant \\(y_i = 0\\) tumour benign. available information patient characteristics, might rely prevalence general population estimate probability. Using risk prediction models, model outcome function observed risk/patient characteristics. risk characteristics contained covariate vector \\(\\boldsymbol{x}_i\\). vector contains observed information patient \\(\\) (e.g. maximum diameter lesion, proportion solid tissue, …). allows us obtain accurate prediction based relation patient characteristics outcome. construct clinical prediction model, either rely statistical models logistic regression machine learning methods. general expression encompasses types models \\[\\begin{align*} E[y_i | \\boldsymbol{x}_i] = f(\\boldsymbol{x}_i). \\end{align*}\\] expression states model response \\(y_i\\) function observed risk characteristics \\(\\boldsymbol{x}_i\\).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"mathematical-details-on-existing-predictive-models","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.1 Risk prediction models","what":"Mathematical details on existing predictive models","title":"Introduction to the CalibrationCurves package","text":"construct risk prediction model, rely logistic regression model \\[\\begin{align*} E[y_i | \\boldsymbol{x}_i] = \\pi_i(\\boldsymbol{\\beta}) = \\frac{e^{\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}}}{1 + e^{\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}}} \\end{align*}\\] \\(\\boldsymbol{\\beta}\\) denotes parameter vector. \\(\\pi_i(\\boldsymbol{\\beta}) = P(y_i = 1| \\boldsymbol{x}_i)\\) denotes probability observing event, given covariate vector \\(\\boldsymbol{x}_i\\). can rewrite equation well-known form \\[\\begin{align*} \\log\\left( \\frac{\\pi_i(\\boldsymbol{\\beta})}{1 - \\pi_i(\\boldsymbol{\\beta})} \\right) &= \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\\\[0.5em] \\text{logit}(\\pi_i(\\boldsymbol{\\beta})) &= \\eta_i \\end{align*}\\] \\(\\eta_i\\) denotes linear predictor. , well-known logit function left side equation. machine learning methods, \\(f(\\cdot)\\) depends specific algorithm. tree-based methods, example, correspond observed proportion leaf nodes. neural networks, \\(f(\\cdot)\\) determined weights layers chosen activation functions.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"different-aspects-of-the-predictive-performance","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models","what":"Different aspects of the predictive performance","title":"Introduction to the CalibrationCurves package","text":"assess well model able predict (probability ) outcome, assess two different aspects model (Van Calster et al. 2016, 2019; Alba et al. 2017): discrimination; calibration. discrimination, refer model’s ability differentiate observations event observations . context, translates giving higher risk estimates patients event patients without event. commonly assess using area receiver operating characteristic curve. However, discrimination performance tell us accurate predictions . estimated risk may result good discrimination can inaccurate time. refer accuracy predictions calibration. Hence, hereby assess agreement estimated observed number events (Van Calster et al. 2016). say prediction model calibrated predicted risks correspond observed proportions event.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"a-mathematical-perspective","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.3 Assessing the calibration performance of a risk prediction model","what":"A mathematical perspective","title":"Introduction to the CalibrationCurves package","text":"One way examine calibration risk predictions, using calibration curves (Van Calster et al. 2016, 2019; Steyerberg 2019; De Cock Campo 2023). calibration curve maps predicted probabilities \\(f(\\boldsymbol{x}_i)\\) actual event probabilities \\(P(y_i = 1| f(\\boldsymbol{x}_i))\\) visualizes correspondence model’s predicted risks true probabilities. perfectly calibrated predictions, calibration curve equals diagonal, .e. \\(P(y_i = 1 | f(\\boldsymbol{x}_i)) = f(\\boldsymbol{x}_i) \\ \\forall \\ \\) \\(\\forall \\ \\) denotes \\(\\).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"a-practical-perspective","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.3 Assessing the calibration performance of a risk prediction model","what":"A practical perspective","title":"Introduction to the CalibrationCurves package","text":"practice, typically assess model’s calibration validation set. setting, calibration curve visualizes correspondence model’s predicted risks observed proportion. perfect agreement observed predicted proportion calibration curve coincides ideal curve (diagonal line). scenario visualized Figure 1.1. Figure 1.1: Example perfectly calibrated model assessing calibration performance data set training set, obtain indication well risk prediction able generalize data sets accurate --sample predictions . general, prediction model show miscalibration calibration curve gives us visual depiction badly model miscalibrated. diagonal line, worse calibration. Figure 1.2 depicts example model miscalibrated typical example model overfitted training data. particular model predictions extreme: high risks overestimated low risks underestimated. Figure 1.2: Example miscalibrated model due overfitting counterpart, underfitted model, occurs less frequently. 1.3 shows calibration curve underfitted model. , overestimation low risks underestimation high risks. Figure 1.3: Example miscalibrated model due underfitting","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"how-do-we-construct-a-calibration-curve","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.3 Assessing the calibration performance of a risk prediction model","what":"How do we construct a calibration curve?","title":"Introduction to the CalibrationCurves package","text":"Fitting logistic regression model training data results estimate parameter vector \\(\\boldsymbol{\\beta}\\), denote \\(\\widehat{\\boldsymbol{\\beta}}\\). latter contains estimated effects included covariates (e.g. proportion solid tissue). obtain risk estimate patient \\(\\), multiply covariate vector \\(\\boldsymbol{x}_i\\) (contains patient-specific characteristics) estimated parameter vector \\(\\widehat{\\boldsymbol{\\beta}}\\) obtain linear predictor \\(\\widehat{\\eta}_i\\) \\[\\begin{align*}   \\widehat{\\eta}_i = \\boldsymbol{x}_i^\\top \\widehat{\\boldsymbol{\\beta}}. \\end{align*}\\] differentiate training test set, append subscript \\(*\\) quantities test set. Hence, \\({}_{*} y_i\\) denotes outcome test set. Similarly, use \\({}_{*} \\boldsymbol{x}_i\\) denote covariate vector patient \\(\\) test set. calculate linear predictor test set \\[\\begin{align*}   {}_{*} \\widehat{\\eta}_i = {}_{*} \\boldsymbol{x}_i^\\top \\widehat{\\boldsymbol{\\beta}} \\tag{1}. \\end{align*}\\] Similarly, can predict probability \\(\\widehat{f}({}_{*} \\boldsymbol{x}_i)\\) patient \\(\\) test set using machine learning methods. use \\[\\begin{align*}   {}_{*} \\widehat{\\pi}_i = \\widehat{f}({}_{*} \\boldsymbol{x}_i) \\end{align*}\\] general notation denote predicted probability risk prediction model. One way compute calibration curve, using logistic regression model \\[\\begin{align*}   \\text{logit}(P({}_{*} y_i = 1| {}_{*} \\widehat{\\pi}_i)) &= \\alpha + \\zeta \\ \\text{logit}({}_{*} \\widehat{\\pi}_i)   \\tag{1.1} \\end{align*}\\] estimate observed proportions function predicted probabilities. model fit yields logistic calibration curve. Note \\(\\text{logit}({}_{*} \\widehat{\\pi}_i) = {}_{*} \\widehat{\\eta}_i\\) \\({}_{*} \\widehat{\\pi}_i\\) estimated using logistic regression model (see (1.1)). Alternatively, can obtain flexible, nonlinear calibration curve using non-parametric smoother loess restricted cubic splines. package, provide types calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"calibration-intercept-and-slope","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.3 Assessing the calibration performance of a risk prediction model","what":"Calibration intercept and slope","title":"Introduction to the CalibrationCurves package","text":"addition calibration curve, two measures summarize different aspects calibration performance: calibration intercept \\(\\alpha_c\\) (calibration---large); calibration slope \\(\\zeta\\). perfectly calibrated model calibration curve coincides diagonal line \\(\\alpha =\\alpha_c = 0\\) \\(\\zeta = 1\\). compute calibration slope \\(\\zeta\\), rely model used obtain logistic calibration curve (see equation (1.1)). value calibration slope \\(\\zeta\\) tells us whether model - underfitted. \\(\\zeta < 1\\) model overfitted. \\(\\zeta < 1\\) indicates \\({}_{*} \\eta_i\\) extreme needs lower ensure predicted risks coincide observed risks. Conversely, model underfitted \\(\\zeta > 1\\). calculate calibration intercept calibration---large, fix calibration slope \\(1\\) denote \\(\\alpha|\\zeta = 1\\) short-hand notation \\(\\alpha_c\\). estimate \\(\\alpha_c\\), fit model \\[\\begin{align*}   \\text{logit}(P({}_{*} y_i = 1| {}_{*} \\widehat{\\pi}_i)) &= \\alpha_c + \\text{offset}(\\text{logit}({}_{*} \\widehat{\\pi}_i))   \\tag{1.2} \\end{align*}\\] enter \\(\\text{logit}({}_{*} \\widehat{\\pi}_i)\\) offset variable. Hereby, fix \\(\\zeta = 1\\). calibration intercept tells us whether risks overestimated \\((\\alpha_c < 0)\\) underestimated \\((\\alpha_c > 0)\\) average.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"training-the-model","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.4 Illustration of the CalibrationCurves package","what":"Training the model","title":"Introduction to the CalibrationCurves package","text":"illustrate functionality, package two example data sets: traindata testdata. two synthetically generated data sets (using underlying process/settings generate data) illustrate functionality CalibrationCurves package. traindata data frame represents data use develop risk prediction model data frame, four covariates one response variable y. Next, fit logistic regression model obtain estimated parameter vector \\(\\widehat{\\beta}\\).","code":"library(CalibrationCurves) #> Loading required package: rms #> Loading required package: Hmisc #>  #> Attaching package: 'Hmisc' #> The following objects are masked from 'package:base': #>  #>     format.pval, units #> Loading required package: ggplot2 #> Registered S3 method overwritten by 'broom': #>   method        from           #>   nobs.multinom riskRegression data(\"traindata\") head(traindata) #>   y          x1         x2         x3         x4 #> 1 0 -0.19981624  0.2982990  1.0277486 -0.1146414 #> 2 1 -1.37127488  0.5940002 -0.8234645  2.0927676 #> 3 1  1.04050541  0.5440481 -1.3576457  1.3126813 #> 4 0 -1.11652476 -0.5382577 -1.1651439  1.0987873 #> 5 1  1.39659613  1.1325081  0.6053029 -1.0598506 #> 6 0 -0.04645095 -0.8167364  1.0196761 -0.4867560 glmFit = glm(y ~ . , data = traindata, family = binomial) summary(glmFit) #>  #> Call: #> glm(formula = y ~ ., family = binomial, data = traindata) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  0.08915    0.08016   1.112    0.266     #> x1           0.60585    0.08475   7.148 8.79e-13 *** #> x2           1.38035    0.10554  13.079  < 2e-16 *** #> x3          -0.75109    0.08854  -8.483  < 2e-16 *** #> x4           0.82757    0.08759   9.448  < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1385.89  on 999  degrees of freedom #> Residual deviance:  950.28  on 995  degrees of freedom #> AIC: 960.28 #>  #> Number of Fisher Scoring iterations: 5"},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"assessing-the-calibration-performance","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.4 Illustration of the CalibrationCurves package","what":"Assessing the calibration performance","title":"Introduction to the CalibrationCurves package","text":"Hereafter, assess calibration performance testdata set. Hereto, first compute predicted probabilities data set. store response testdata separate vector yTest. Now everything need assess calibration performance prediction model. can either use val.prob.ci.2 valProbggplot visualize calibration performance obtain statistics. val.prob.ci.2 makes plot using base R valProbggplot uses ggplot2 package. default, flexible calibration curve (based loess smoother) plotted. addition plot, function returns object class CalibrationCurve. object contains calculated statistics well calculated coordinates calibration curve. coordinates stored CalibrationCurves slot can extracted follows.  Alternatively, can use restricted cubic splines obtain flexible calibration curve.  obtain logistic calibration curve using following code.  can plot using  package also allows change colors, change position legend much . Check help-function see arguments functions .  Finally, can also decide statistics appear plot.","code":"data(\"testdata\") pHat = predict(glmFit, newdata = testdata, type = \"response\") yTest = testdata$y calPerf = val.prob.ci.2(pHat, yTest) calPerf #> Call: #> val.prob.ci.2(p = pHat, y = yTest) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.62853462   0.81426731   0.38019823   0.33282644 167.41322219   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01286390   8.43195136   0.01475792   0.31996254   0.17703339   0.22404680  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.82278297   0.08288689   0.28730517   0.04747448   0.32064056 str(calPerf) #> List of 7 #>  $ call             : language val.prob.ci.2(p = pHat, y = yTest) #>  $ stats            : Named num [1:17] 0.629 0.814 0.38 0.333 167.413 ... #>   ..- attr(*, \"names\")= chr [1:17] \"Dxy\" \"C (ROC)\" \"R2\" \"D\" ... #>  $ cl.level         : num 0.95 #>  $ Calibration      :List of 2 #>   ..$ Intercept: Named num [1:3] 0.22405 0.00339 0.4447 #>   .. ..- attr(*, \"names\")= chr [1:3] \"Point estimate\" \"Lower confidence limit\" \"Upper confidence limit\" #>   ..$ Slope    : Named num [1:3] 0.823 0.666 0.98 #>   .. ..- attr(*, \"names\")= chr [1:3] \"Point estimate\" \"Lower confidence limit.2.5 %\" \"Upper confidence limit.97.5 %\" #>  $ Cindex           : Named num [1:3] 0.814 0.774 0.848 #>   ..- attr(*, \"names\")= chr [1:3] \"Point estimate\" \"Lower confidence limit\" \"Upper confidence limit\" #>  $ warningMessages  : NULL #>  $ CalibrationCurves:List of 1 #>   ..$ FlexibleCalibration:'data.frame':  500 obs. of  4 variables: #>   .. ..$ x   : num [1:500] 0.00561 0.00651 0.00706 0.01183 0.01235 ... #>   .. ..$ y   : num [1:500] 0.0504 0.0514 0.052 0.0572 0.0578 ... #>   .. ..$ ymin: num [1:500] 0 0 0 0 0 0 0 0 0 0 ... #>   .. ..$ ymax: num [1:500] 0.177 0.177 0.177 0.179 0.179 ... #>  - attr(*, \"class\")= chr \"CalibrationCurve\" flexCal = calPerf$CalibrationCurves$FlexibleCalibration plot(flexCal[, 1:2], type = \"l\", xlab = \"Predicted probability\", ylab = \"Observed proportion\", lwd = 2, xlim = 0:1, ylim = 0:1) polygon(   x = c(flexCal$x, rev(flexCal$x)),   y = c(     flexCal$ymax,     rev(flexCal$ymin)   ),   col = rgb(177, 177, 177, 177, maxColorValue = 255),   border = NA ) rcsFit = tryCatch(val.prob.ci.2(pHat, yTest, smooth = \"rcs\"),                   error = function(e) TRUE) if(is.logical(rcsFit)) {   plot(1, type = \"n\", xlab = \"\", ylab = \"\", xlim = c(0, 10), ylim = c(0, 10))   text(x = 5, y = 5, labels = paste0(\"There was a problem estimating\\n\",                                      \"the calibration curve using rcs\"), cex = 2) } else {   rcsFit } #> Call: #> val.prob.ci.2(p = pHat, y = yTest, smooth = \"rcs\") #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.62853462   0.81426731   0.38019823   0.33282644 167.41322219   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01286390   8.43195136   0.01475792   0.31996254   0.17703339   0.22404680  #>        Slope         Emax Brier scaled  #>   0.82278297   0.08288689   0.28730517 invisible(val.prob.ci.2(pHat, yTest, logistic.cal = TRUE, smooth = \"none\")) invisible(val.prob.ci.2(pHat, yTest, logistic.cal = TRUE, col.log = \"orange\")) invisible(val.prob.ci.2(pHat, yTest, col.ideal = \"black\", col.smooth = \"red\", CL.smooth = TRUE,               legendloc = c(0, 1), statloc = c(0.6, 0.25))) invisible(val.prob.ci.2(pHat, yTest, dostats = c(\"C (ROC)\", \"Intercept\", \"Slope\", \"ECI\")))"},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"ggplot-version","dir":"Articles","previous_headings":"1 Assessing the performance of risk prediction models > 1.4 Illustration of the CalibrationCurves package","what":"ggplot version","title":"Introduction to the CalibrationCurves package","text":"ggplot version (.e.valProbggplot) uses virtually arguments. Hence, can easily obtain ggplot using code.","code":"valProbggplot(pHat, yTest) #> Call: #> valProbggplot(p = pHat, y = yTest) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.62853462   0.81426731   0.38019823   0.33282644 167.41322219   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01286390   8.43195136   0.01475792   0.31996254   0.17703339   0.22404680  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.82278297   0.08288689   0.28730517   0.04747448   0.32064056"},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"cox-proportional-hazards-model","dir":"Articles","previous_headings":"2 Assessing the performance of survival models","what":"Cox Proportional hazards model","title":"Introduction to the CalibrationCurves package","text":"Cox proportional hazards model widely used method analyzing survival data. estimates hazard function, represents instantaneous risk event occurring time \\(t\\), given subject survived \\(t\\). patient \\(\\) covariate vector \\(\\boldsymbol{x}_i\\), hazard function given : \\[\\begin{align*}   h(t | \\boldsymbol{x}_i) &= h_0(t) \\exp(\\boldsymbol{x}_i^\\top \\beta)\\\\   &= h_0(t) \\exp(\\eta_i) \\end{align*}\\] \\(h(t | \\boldsymbol{x}_i)\\) hazard function time \\(t\\) subject \\(\\), \\(h_0(t)\\) baseline hazard function time \\(t\\), shared individuals. model, survival probability individual \\(\\) time \\(t\\) given \\[ S_i(t) = \\exp\\left(-H_i(t)\\right) \\] \\(H_i(t)\\) cumulative hazard function: \\[ H_i(t) = H_0(t) \\exp( \\eta_i) \\] \\(H_0(t)\\) baseline cumulative hazard function.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"calibration-curve","dir":"Articles","previous_headings":"2 Assessing the performance of survival models > 2.1 Cox Proportional hazards model","what":"Calibration Curve","title":"Introduction to the CalibrationCurves package","text":"can estimate calibration curve scenario entering linear predictor \\(\\eta_i\\) covariate\\[\\begin{align*}   h(t | {}_{*} \\eta_i) &= h_0(t) \\exp(\\zeta {}_{*} \\eta_i)\\\\ \\end{align*}\\] \\(\\zeta\\) calibration slope quantifies relationship predicted observed hazards. Similarly (1.1), \\(\\zeta\\) indicates whether model - (\\(\\zeta < 1\\)) underfitted (\\(\\zeta >1\\)). Note calibration intercept model. baseline hazard function \\(h_0(t)\\) estimated Cox model, model identified proportionality constant. result, intercept term absorbed baseline hazard function, separately identifiable. explicitly specify time point \\(t\\), calibration can assessed either across follow-time points specific time point interest. estimate model, can fit Cox proportional hazards model. short example illustrating can assess calibration performance Cox proportional hazards model using package. , first fit model using coxph survival package. assess calibration performance, pass object containing model fit along external validation data set valProbSurvival function.  Next plot, also get range statistics assess calibration performance.","code":"library(survival) data(trainDataSurvival) data(testDataSurvival) sFit = coxph(Surv(ryear, rfs) ~ csize + cnode + grade3, data = trainDataSurvival,              x = TRUE, y = TRUE) calPerf = valProbSurvival(sFit, testDataSurvival, plotCal = \"ggplot\", nk = 5) calPerf #> Call: #> valProbSurvival(fit = sFit, valdata = testDataSurvival, nk = 5,  #>     plotCal = \"ggplot\") #>  #> A 95% confidence interval is given for the statistics.  #>  #> Calibration performance: #> ------------------------ #>  #> In the large #>  #>        OE     2.5 %    97.5 %  #> 1.0444489 0.9299645 1.1730270  #>  #> Slope #>  #> calibration slope             2.5 %            97.5 %  #>         1.0703257         0.8202242         1.3204271  #>  #> Additional statistics #>  #>        ICI        E50        E90       Emax  #> 0.02844123 0.04046305 0.05838470 0.05857903  #>         model times     Brier           se     lower     upper       IPA #>        <fctr> <num>     <num>        <num>     <num>     <num>     <num> #> 1: Null model  4.99 0.2499302 0.0004004949 0.2491452 0.2507151 0.0000000 #> 2:        cox  4.99 0.2245471 0.0077937209 0.2092717 0.2398225 0.1015608 #>  #>  #> Discrimination performance: #> ------------------------------- #>  #> Concordance statistic #>  #>            Estimate     2.5 %    97.5 % #> Harrell C 0.6517240 0.6193261 0.6841220 #> Uno C     0.6388712 0.6071328 0.6706096 #>  #>  #> Time-dependent AUC #>  #>   Uno AUC     2.5 %   97. 5 %  #> 0.6856354 0.6305826 0.7406882"},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"assessing-the-calibration-of-prediction-models-in-clustered-datasets","dir":"Articles","previous_headings":"","what":"Assessing the calibration of prediction models in clustered datasets","title":"Introduction to the CalibrationCurves package","text":"certain scenarios, hierarchically structured data. , heterogeneity clusters dependency among observations belonging cluster. utmost importance hierarchical structure taken account assessing calibration predictive model. prime example hierarchical structure patients (level 1) nested within hospitals (level 2). cases, observations within cluster may similar belonging different clusters. assume validation data set total \\(J\\) clusters use \\(j = (1, \\dots, J)\\) index clusters. cluster, \\(n_j\\) patients index patients using \\(= (1, \\dots, n_j)\\). total sample size \\(N = \\sum_{j = 1}^J n_j\\). use \\(y_{ij} \\(0,1)\\) denote outcome patient \\(\\) cluster \\(j\\), takes value 0 case non-event 1 case event. assume \\(y_{ij}\\) follows Bernouilli distribution \\(y_{ij} \\sim \\text{Bern}(\\pi_{ij})\\), \\(\\pi_{ij}\\) denotes probability experiencing event. Similar , express \\(\\pi_{ij}\\) function set risk characteristics \\[ P(y_{ij} = 1 | \\boldsymbol{x}_{ij}) = \\pi(\\boldsymbol{x}_{ij}). \\] simple logistic regression model, \\[ \\pi(\\boldsymbol{x}_{ij}) = \\frac{1}{1 + e^{-\\boldsymbol{x}_{ij}^\\top \\boldsymbol{\\beta}}}. \\] perfectly calibrated model \\(P(y_{ij} = 1 | \\pi(\\boldsymbol{x}_{ij})) = \\pi(\\boldsymbol{x}_{ij})\\) \\(\\) \\(j\\). estimate calibration, extend (1.1) \\[   \\text{logit}(P({}_{*} y_{ij} = 1| {}_{*} \\widehat{\\pi}_{ij})) = \\alpha + \\zeta \\ \\text{logit}({}_{*} \\widehat{\\pi}_{ij}). \\] However, approach, ignore clustering obtain information performance different clusters. Since commonly accepted way estimate calibration curves scenario, Barreñada et al. (2025) propose three approaches obtain calibration curves: Clustered Group Calibration (CG-C): bivariate random effects meta-analysis model; Two-stage Neta-Analysis Calibration (2MA-C): two-stage univariate random effect meta-analytical approach; Mixed model Calibration (MIX-C): one-step approach using random-effects model. three approaches implemented CalibrationCurves package can access valProbCluster function. illustrate methods, use clustered training validation data set.","code":"library(lme4) #> Loading required package: Matrix data(\"clustertraindata\") data(\"clustertestdata\") mFit           = glmer(y ~ x1 + x2 + x3 + x5 + (1 | cluster), data = clustertraindata, family = \"binomial\") preds          = predict(mFit, clustertestdata, type = \"response\", re.form = NA) y              = clustertestdata$y cluster        = clustertestdata$cluster valClusterData = data.frame(y = y, preds = preds, center = cluster)"},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"CGC","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets","what":"Clustered Group Calibration (CG-C)","title":"Introduction to the CalibrationCurves package","text":"CG-C method extension traditional grouped calibration method (binning calibration) data sets clustered structure. traditional grouped calibration: data pooled; observations divided equal-sized groups based predicted risks; group, observed event rate plotted mean predicted risk. Conversely, CG-C: grouping performed within cluster first; cluster-level results combined across clusters via bivariate random-effects meta-analysis, capturing -cluster heterogeneity within-cluster sampling error. package, two grouping strategies available: \"grouped\": equal-sized quantile groups within cluster; \"interval\": fixed-width intervals (0, 1) within cluster.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"mathematical-details","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.1 Clustered Group Calibration (CG-C)","what":"Mathematical details","title":"Introduction to the CalibrationCurves package","text":"CG-C method proceeds follows: Per cluster \\(j\\), split predicted probabilities \\(Q\\) groups; Within group \\(q\\) cluster \\(j\\), compute mean observed outcome (\\(\\hat{y}_{qj}\\)) mean predicted probability (\\(\\hat{\\bar{\\pi}}_{qj}\\)). Per group \\(q\\), perform bivariate random-effects meta-analysis. include \\(\\text{logit}(\\hat{y}_{qj})\\) \\(\\text{logit}(\\hat{\\bar{\\pi}}_{qj})\\) response variables capture cluster-specific deviation including random intercept. fitting model, use unstructured variance–covariance matrix. equation model given \\[ \\begin{bmatrix} \\text{logit}(\\bar{y}_{qj}) \\\\ \\text{logit}(\\hat{\\bar{\\pi}}_{qj}) \\end{bmatrix} = \\begin{bmatrix} {}_{\\bar{y}} \\mu_q + u_{qj} + \\varepsilon_{qj} \\\\ {}_{\\hat{\\bar{\\pi}}} \\mu_q + \\nu_{qj} + \\epsilon_{qj} \\end{bmatrix} \\tag{2} \\] \\(\\bar{y}_{qj}\\) denotes prevalence cluster \\(j\\) within quantile \\(q\\), \\(\\hat{\\bar{\\pi}}_{qj}\\) average estimated risk cluster \\(j\\) within quantile \\(q\\). use subscript \\(q\\) quantities indicate refers quantile \\(q\\). \\(u_{qj}\\) \\(\\nu_{qj}\\) represent random intercepts cluster \\(j\\), capturing -cluster heterogeneity, whereas \\(\\varepsilon_{qj}\\) \\(\\epsilon_{qj}\\) account within-cluster error. model, assume \\({}_{\\bar{y}} \\mu_q + u_{qj}\\) \\({}_{\\hat{\\bar{\\pi}}} \\mu_q + \\nu_{qj}\\) randomly drawn distribution mean (pooled) \\({}_{\\bar{y}} \\mu_q\\) \\({}_{\\hat{\\bar{\\pi}}} \\mu_q\\), respectively. \\[ \\begin{bmatrix} u_{qj} \\\\ \\nu_{qj} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\Omega_q \\right) \\quad \\text{} \\quad \\begin{bmatrix} \\varepsilon_{qj} \\\\ \\epsilon_{qj} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\Sigma_{qj} \\right) \\] \\(\\Omega_q\\) \\(\\Sigma_{qj}\\) represent -cluster within-cluster covariance matrices within quantile \\(q\\), respectively. \\[ \\Omega_q = \\begin{bmatrix} {}_{\\bar{y}} \\tau_q^2 & {}_{b} \\rho_q \\ \\ \\ {}_{\\bar{y}} \\tau_q^2 \\ \\ {}_{\\bar{\\hat{\\pi}}} \\tau_q^2 \\\\ {}_{b} \\rho_q \\ \\ \\ {}_{\\bar{y}} \\tau_q^2 \\ \\ {}_{\\bar{\\hat{\\pi}}} \\tau_q^2 & {}_{\\bar{\\hat{\\pi}}} \\tau_q^2 \\end{bmatrix} \\] \\[ \\Sigma_{qj} = \\begin{bmatrix} {}_{\\bar{y}} \\sigma_{qj}^2 & {}_{w} \\rho_q \\ \\ {}_{\\bar{y}} \\sigma_{qj}^2 \\ \\ {}_{\\bar{\\hat{\\pi}}} \\sigma_{qj}^2 \\\\ {}_{w} \\rho_q \\ \\ {}_{\\bar{y}} \\sigma_{qj}^2 \\ \\ {}_{\\bar{\\hat{\\pi}}} \\sigma_{qj}^2 & {}_{\\bar{\\hat{\\pi}}} \\sigma_{qj}^2 \\end{bmatrix} \\] use \\({}_{\\bar{y}} \\sigma_{qj}^2\\) \\({}_{\\bar{\\hat{\\pi}}} \\sigma_{qj}^2\\) denote variance \\(\\bar{y}_{qj}\\) \\(\\hat{\\bar{\\pi}}_{qj}\\), respectively. \\({}_{w} \\rho_q\\) represents within-cluster correlation \\({}_{b} \\rho_q\\) -cluster correlation quantile \\(q\\). -study variance denoted \\({}_{\\bar{\\hat{\\pi}}} \\tau_q^2\\) \\({}_{\\bar{\\hat{\\pi}}} \\tau_q^2\\). quantile \\(q\\), use bivariate meta-analysis model account strong dependence average estimated risk observed proportion. calibration plot created plotting \\(Q\\) values \\({}_{\\bar{\\hat{\\pi}}} \\mu_q\\) x-axis \\({}_{\\bar{y}} \\mu_q\\) y-axis.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"illustration","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.1 Clustered Group Calibration (CG-C)","what":"Illustration","title":"Introduction to the CalibrationCurves package","text":"following code gives example CG-C (Grouped).  can employ CG-C (Interval) using following code. follows steps Grouped method, except Step 1 uses equally spaced probability intervals instead quantiles.  interpret plot follows: Black: Traditional grouped calibration curve (pooled data) Gold: CG-C pooled curve across clusters Light blue = 95% Prediction Interval (expected calibration curve new clusters) Cornflower blue = 95% Confidence Interval (expected calibration pooled curve) CG-C provides calibration estimates account cluster-level variability, making representative model performance new unseen clusters.","code":"res_cgcg <- valProbCluster(   data = valClusterData,   p = preds, y = y, cluster = center,   approach = \"CGC\", method = \"grouped\" ) res_cgcg #> Call: #> valProbCluster(data = valClusterData, p = preds, y = y, cluster = center,  #>     approach = \"CGC\", method = \"grouped\") #>  #> A 95% confidence interval is used. res_cgci <- valProbCluster(   data = valClusterData,   p = preds, y = y, cluster = center,   approach = \"CGC\", method = \"interval\" ) #> Warning in (function (data = NULL, p, y, cluster, cl.level = 0.95, ntiles = 10, #> : There is a group with less than 50 observations. Consider decreasing the #> number of groups. res_cgci #> Call: #> valProbCluster(data = valClusterData, p = preds, y = y, cluster = center,  #>     approach = \"CGC\", method = \"interval\") #>  #> A 95% confidence interval is used."},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"id_2MAC","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets","what":"Two stage meta-analysis calibration (2MA-C)","title":"Introduction to the CalibrationCurves package","text":"second approach, use two-stage random effects meta-analysis estimate calibration plot. First, fit flexible curve per cluster smoother choice. Currently, user can choose restricted cubic splines LOESS. employing restricted cubic splines, number knots per cluster selected performing likelihood ratio test combinations models 3, 4, 5 knots, selecting model least knots provides best fit. opt LOESS, span parameter lowest bias-corrected AIC selected cluster.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"mathematical-details-1","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.2 Two stage meta-analysis calibration (2MA-C)","what":"Mathematical details","title":"Introduction to the CalibrationCurves package","text":"train flexible curve per cluster estimate observed proportion fixed grid estimated risks 0.01 0.99 (\\(G\\), default 100 points). Hereafter, use random effects meta-analysis model per point grid combine logit-transformed predictions across clusters, fitting following univariate model per point \\(g\\) grid: \\[ \\text{logit}({}_{s} \\hat{\\pi}_{gj}) = {}_{\\hat{\\pi}} \\mu_g + \\nu_{gj} + \\epsilon_{gj} \\] \\({}_{s} \\hat{\\pi}_{gj}\\) denotes predicted proportion point \\(g\\) cluster \\(j\\), \\({}_{\\hat{\\pi}} \\mu_g\\) overall mean point \\(g\\), \\(\\nu_{gj}\\) cluster-specific deviation, \\(\\epsilon_{gj}\\) error. assume \\(\\nu_{gj} \\sim \\mathcal{N}(0, {}_{\\hat{\\pi}} \\tau_g^2)\\) \\(\\epsilon_{gj} \\sim \\mathcal{N}(0, {}_{\\hat{\\pi}} \\sigma_{gj}^2)\\). , include inverse variance \\(\\text{logit}({}_{s} \\hat{\\pi}_{gj})\\) weight hereby take cluster-specific (\\({}_{\\hat{\\pi}} \\sigma_{gj}^2\\)) -cluster variability (\\({}_{\\hat{\\pi}} \\tau_g^2\\)) account.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"illustration-1","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.2 Two stage meta-analysis calibration (2MA-C)","what":"Illustration","title":"Introduction to the CalibrationCurves package","text":"code shows estimate calibration curves using MAC2 method, using splines LOESS first step.   interpret plot follows: Black (dashed): 2MA-C pooled curve across clusters Black (solid): Center specific curves (center_curves = TRUE) Light red = 95% Prediction Interval (expected calibration curve new clusters) Dark red = 95% Confidence Interval (expected calibration pooled curve) 2MA-C method provides calibration estimates account cluster-level variability, making representative model performance new unseen clusters time calculate center specific calibration curves. Note example clustering based normally distributed random effect, might case real life scenarios. Due , splines model seems fit better loess model. However random effects follows complex structure, loess model might appropriate.","code":"res_mac2_spl <- valProbCluster(   data = valClusterData,   p = preds, y = y, cluster = center,   approach = \"MAC2\", knots = 3,   method_choice = \"splines\" ) res_mac2_spl #> Call: #> valProbCluster(data = valClusterData, p = preds, y = y, cluster = center,  #>     approach = \"MAC2\", knots = 3, method_choice = \"splines\") #>  #> A 95% confidence interval is used. res_mac2_loess <- valProbCluster(   data = valClusterData,   p = preds, y = y, cluster = center,   grid_l = 20,   approach = \"MAC2\",   method_choice = \"loess\" ) res_mac2_loess #> Call: #> valProbCluster(data = valClusterData, p = preds, y = y, cluster = center,  #>     approach = \"MAC2\", grid_l = 20, method_choice = \"loess\") #>  #> A 95% confidence interval is used."},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"MIXC","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets","what":"Mixed model calibration (MIX-C)","title":"Introduction to the CalibrationCurves package","text":"third approach, employ one-stage logistic generalized linear mixed model (GLMM) model outcome function logit-transformed predictions. allow non-linear effect, employ restricted cubic splines three knots fixed random effects \\[ \\text{logit}\\left(P(y_{ij} = 1 \\mid \\widehat{\\pi}(\\boldsymbol{x}_{ij}), \\widetilde{s}_j)\\right) = s\\left(\\text{logit}\\left(\\widehat{\\pi}(\\boldsymbol{x}_{ij})\\right)\\right) + \\tilde{s}_j \\left(\\text{logit}\\left(\\widehat{\\pi}(\\boldsymbol{x}_{ij})\\right)\\right). \\] , \\(s\\) denotes smooth effect \\(\\tilde{s}_j\\) denotes smooth random effect cluster \\(j\\). model fit using package, splines added package. approach estimates calibration per cluster variance random effects single step.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"mathematical-details-2","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.3 Mixed model calibration (MIX-C)","what":"Mathematical details","title":"Introduction to the CalibrationCurves package","text":"prediction intervals calculated using predictInterval function R 10,000 samples (simulation-based). function takes following account: uncertainty observation level (residual variance); fixed coefficients; random effects. method: first obtain random fixed effects. generate \\(n\\) samples (default = 10,000) based multivariate normal distribution random fixed effects, separately. calculate linear predictor sample. predict upper lower limits prediction interval. Detailed methodological overview can found published article.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"illustration-2","dir":"Articles","previous_headings":"3 Assessing the calibration of prediction models in clustered datasets > 3.3 Mixed model calibration (MIX-C)","what":"Illustration","title":"Introduction to the CalibrationCurves package","text":"Using code shown , can estimate calibration curve using MIX-C method.  interpret plot follows: Black (dashed): MIX-C pooled curve across clusters Black (solid): Center specific curves (center_curves = TRUE) Light green = 95% Prediction Interval (expected calibration curve new clusters) Dark green = 95% Confidence Interval (expected calibration pooled curve can calculated Delta method normal approximation) MIX-C mixed-effects logistic calibration model using restricted cubic splines random intercepts slopes per cluster, designed account fixed random effects calibration. performs especially well cluster-specific calibration curves, particularly sample sizes per cluster small, due shrinkage properties. limitation prediction intervals can inconsistently narrow wide, like methods, tends underestimate -cluster heterogeneity. Overall, MIX-C recommended producing cluster-specific calibration plots, methods may preferred average-effect curves.","code":"res_mixc_slo <- valProbCluster(   p = valClusterData$preds, y = valClusterData$y, cluster = valClusterData$center,   pl = TRUE,   approach = \"MIXC\", method = \"slope\" ) res_mixc_slo #> Call: #> valProbCluster(p = valClusterData$preds, y = valClusterData$y,  #>     cluster = valClusterData$center, plot = TRUE, approach = \"MIXC\",  #>     method = \"slope\") #>  #> A 95% confidence interval is used."},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"assessing-the-performance-of-other-types-of-prediction-models","dir":"Articles","previous_headings":"","what":"Assessing the performance of other types of prediction models","title":"Introduction to the CalibrationCurves package","text":"recent paper (De Cock Campo 2023), propose extension logistic calibration framework distributions belong exponential family probability density function (pdf) \\[\\begin{align*}         f(y_i; \\theta_i, \\phi, w_i) = \\exp\\left( \\frac{y_i \\theta_i - b(\\theta_i)}{\\phi} w_i  + c(y_i, \\phi, w_i)\\right). \\end{align*}\\] , \\(\\theta_i\\) natural parameter, \\(\\phi\\) dispersion parameter \\(w_i\\) weight. \\(b(\\cdot)\\) \\(c(\\cdot)\\) known functions. Similar , assume unknown regression function \\(r(\\boldsymbol{x}_i) = E[y_i | \\boldsymbol{x}_i]\\). approximate unknown function, rely prediction models following functional form \\[\\begin{align*}     E[y_i | \\boldsymbol{x}_i] = \\mu_i = f(\\boldsymbol{x}_i).     \\tag{4.1} \\end{align*}\\] estimate (4.1), can use generalized linear model \\[\\begin{align*}         g(E[y_i | \\boldsymbol{x}_i]) = \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta} = \\eta_i.         \\tag{4.2} \\end{align*}\\] \\(g(\\cdot)\\) denotes link function. Alternatively, can estimate (4.1) using machine learning methods. Using model fit, obtain predictions \\(\\widehat{\\mu}_i = \\widehat{f}(\\boldsymbol{x}_i)\\).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"generalized-calibration-curves","dir":"Articles","previous_headings":"4 Assessing the performance of other types of prediction models","what":"Generalized calibration curves","title":"Introduction to the CalibrationCurves package","text":"examine calibration prediction models outcome member exponential family, redefine framework general terms. context, calibration curve maps predicted values \\(f(\\boldsymbol{x}_i)\\) \\(E[y_i| f(\\boldsymbol{x}_i)]\\), actual conditional mean \\(y_i\\) given \\(f(\\boldsymbol{x}_i)\\). , model perfectly calibrated calibration curve equals diagonal, .e. \\(E[y_i | f(\\boldsymbol{x}_i)] = f(\\boldsymbol{x}_i) \\ \\forall \\ \\). Hence, context, calibration curve captures correspondence predicted values conditional mean. propose two methods estimate calibration curve. Firstly, can estimate calibration curve using generalized linear model \\[\\begin{align*}         g(E[{}_{*} y_i | {}_{*} \\widehat{\\mu}_i]) = \\alpha + \\zeta \\ g({}_{*} \\widehat{\\mu}_i).         \\tag{4.3} \\end{align*}\\] transforming \\({}_{*} \\widehat{\\mu}_i\\) using appropriate \\(g(\\cdot)\\), map \\({}_{*} \\widehat{\\mu}_i\\) whole real line better fit model. \\({}_{*} \\widehat{\\mu}_i\\) estimated using generalized linear model link function (.e. \\(g(\\cdot)\\) identical (4.2) (4.3)), follows \\(g({}_{*} \\widehat{\\mu}_i) = {}_{*} \\widehat{\\eta}_i\\). Using equation (4.3), estimate empirical average function predicted values. , similarly (1.1), \\(\\zeta\\) tells us whether model - (\\(\\zeta < 1\\)) underfitted (\\(\\zeta >1\\)). estimate calibration---large \\(\\alpha_c\\) \\[\\begin{align*}         g(E[{}_{*} y_i | {}_{*} \\widehat{\\mu}_i]) = \\alpha_c + \\text{offset}(g({}_{*} \\widehat{\\mu}_i)).         \\tag{4.4} \\end{align*}\\] Hereby, assess extent observed empirical average equals average predicted value. Secondly, logistic regression model, can employ non-parametric smoothers estimate calibration curve.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"training-the-model-1","dir":"Articles","previous_headings":"4 Assessing the performance of other types of prediction models > 4.2 Illustration of the generalized calibration framework","what":"Training the model","title":"Introduction to the CalibrationCurves package","text":"illustrate functionality, package two example data sets poisson distributed outcome variable: poissontraindata poissontestdata. two synthetically generated data sets (using underlying process/settings generate data) illustrate functionality CalibrationCurves package. poissontraindata data frame represents data use develop prediction model. data frame, five covariates one response variable y. Next, fit Poisson GLM log link obtain estimated parameter vector \\(\\widehat{\\beta}\\).","code":"data(\"poissontraindata\") head(traindata) #>   y          x1         x2         x3         x4 #> 1 0 -0.19981624  0.2982990  1.0277486 -0.1146414 #> 2 1 -1.37127488  0.5940002 -0.8234645  2.0927676 #> 3 1  1.04050541  0.5440481 -1.3576457  1.3126813 #> 4 0 -1.11652476 -0.5382577 -1.1651439  1.0987873 #> 5 1  1.39659613  1.1325081  0.6053029 -1.0598506 #> 6 0 -0.04645095 -0.8167364  1.0196761 -0.4867560 glmFit = glm(Y ~ . , data = poissontraindata, family = poisson) summary(glmFit) #>  #> Call: #> glm(formula = Y ~ ., family = poisson, data = poissontraindata) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -2.33425    0.05301 -44.034  < 2e-16 *** #> x1           1.28147    0.17645   7.262 3.80e-13 *** #> x2           2.02783    0.18019  11.254  < 2e-16 *** #> x3          -1.16815    0.16907  -6.909 4.88e-12 *** #> x4          -1.88795    0.17840 -10.583  < 2e-16 *** #> x5          -1.84003    0.17866 -10.299  < 2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for poisson family taken to be 1) #>  #>     Null deviance: 3087.4  on 4999  degrees of freedom #> Residual deviance: 2660.4  on 4994  degrees of freedom #> AIC: 4053.3 #>  #> Number of Fisher Scoring iterations: 6"},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"assessing-the-calibration-performance-1","dir":"Articles","previous_headings":"4 Assessing the performance of other types of prediction models > 4.2 Illustration of the generalized calibration framework","what":"Assessing the calibration performance","title":"Introduction to the CalibrationCurves package","text":"Hereafter, assess calibration performance poissontestdata set. Hereto, first compute predicted values data set. store response poissontestdata separate vector yTest. Now everything need assess calibration performance prediction model. can use genCalCurve visualize calibration performance obtain statistics. genCalCurve makes plot using base R ggplot version included one next updates. default, calibration curve estimated GLM plotted. , addition outcome predicted values, specify distribution response variable. addition plot, function returns object class GeneralizedCalibrationCurve. object contains calculated statistics well calculated coordinates calibration curve. coordinates stored CalibrationCurves slot can extracted follows.","code":"data(\"poissontestdata\") yHat = predict(glmFit, newdata = poissontestdata, type = \"response\") yTest = poissontestdata$Y calPerf = genCalCurve(yTest, yHat, family = poisson) #> Waiting for profiling to be done... #> Waiting for profiling to be done... calPerf #> Call: #> genCalCurve(y = yTest, yHat = yHat, family = poisson) #>  #> A 95% confidence interval is given for the calibration intercept and calibration slope.  #>  #> Calibration intercept     Calibration slope  #>            0.02710876            0.98320991 str(calPerf) #> List of 6 #>  $ call             : language genCalCurve(y = yTest, yHat = yHat, family = poisson) #>  $ stats            : Named num [1:2] 0.0271 0.9832 #>   ..- attr(*, \"names\")= chr [1:2] \"Calibration intercept\" \"Calibration slope\" #>  $ cl.level         : num 0.95 #>  $ Calibration      :List of 2 #>   ..$ Intercept: Named num [1:3] 0.0271 -0.1368 0.1825 #>   .. ..- attr(*, \"names\")= chr [1:3] \"Point estimate.Calibration intercept\" \"Lower confidence limit.2.5 %\" \"Upper confidence limit.97.5 %\" #>   ..$ Slope    : Named num [1:3] 0.983 0.768 1.198 #>   .. ..- attr(*, \"names\")= chr [1:3] \"Point estimate.Calibration slope\" \"Lower confidence limit.2.5 %\" \"Upper confidence limit.97.5 %\" #>  $ warningMessages  : NULL #>  $ CalibrationCurves:List of 1 #>   ..$ GLMCalibration:'data.frame':   1000 obs. of  2 variables: #>   .. ..$ x: num [1:1000] 0.0111 0.012 0.0124 0.0142 0.0149 ... #>   .. ..$ y: num [1:1000] 0.012 0.013 0.0134 0.0152 0.0159 ... #>  - attr(*, \"class\")= chr \"GeneralizedCalibrationCurve\" GLMCal = calPerf$CalibrationCurves$GLMCalibration plot(GLMCal[, 1:2], type = \"l\", xlab = \"Predicted value\", ylab = \"Empirical average\", lwd = 2, xlim = 0:1, ylim = 0:1,      col = \"red\", lty = 2) abline(0, 1, lty = 1)"},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"why-is-the-calibration-intercept-different-in-the-rms-package","dir":"Articles","previous_headings":"5 FAQ","what":"Why is the calibration intercept different in the rms package?","title":"Introduction to the CalibrationCurves package","text":"construct logistic calibration curve (see construct calibration curve?), fit model \\[\\begin{align*} \\text{logit}(E[{}_{*} y_i | {}_{*} \\widehat{\\pi}_i]) = \\alpha + \\zeta \\ \\text{logit}({}_{*} \\widehat{\\pi}_i) \\end{align*}\\] , \\(\\zeta\\) corresponds calibration slope. calibration intercept val.prob function rms package corresponds \\(\\alpha \\neq \\alpha_c\\). CalibrationCurves package, calibration intercept corresponds \\(\\alpha_c\\) assesses calibration large. Using formulation, calibration intercept indicates whether predicted risks - overestimated average conform definition calibration intercept article ‘calibration hierarchy risk models defined: utopia empirical data’ (articles published topic) (Van Calster et al. 2016, 2019). compute \\(\\alpha_c\\) using \\[\\begin{align*} \\text{logit}(E[{}_{*} y_i | {}_{*} \\widehat{\\pi}_i]) = \\alpha_c + \\text{offset}(\\text{logit}({}_{*} \\widehat{\\pi}_i)). \\end{align*}\\] fix \\(\\zeta = 1\\) including \\(\\text{logit}({}_{*} \\widehat{\\pi}_i)\\) offset variable. Consequently, types calibration intercepts need interpreted differently: corresponds constant add multiplied linear predictor ‘correction’ factor (.e. calibration slope) get predicted probabilities correspond observed ones. essence: multiplied linear predictor correction factor, constant still add make predicted probabilities correspond observed ones? \\(> 0\\): \\({}_{*} \\widehat{\\pi}_i\\) low average hence, average risks underestimated. increase make correspond observed probabilities. \\(< 0\\): \\({}_{*} \\widehat{\\pi}_i\\) high average hence, average risks overestimated. decrease make correspond observed probabilities.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/articles/CalibrationCurves.html","id":"i-have-predicted-probabilities-of-0-or-1--why-is-this-not-allowed-by-default-and-why-do-i-get-these-annoying-warning-messages","dir":"Articles","previous_headings":"5 FAQ","what":"I have predicted probabilities of 0 or 1. Why is this not allowed by default and why do I get these annoying warning messages?","title":"Introduction to the CalibrationCurves package","text":"Predicted probabilities 0 1 imply randomness process deterministic. process truly deterministic, model . Mostly presence perfect predictions signifies something went wrong fitting model model severely overfitted. therefore make sure allowed default delete observations. observe behavior following cases:  - logistic regression: quasi-complete separation, coefficients tend infinity;  - tree-based methods: one leaf nodes contains observations either 0 1;  - neural networks: weights tend infinity known weight/gradient explosion. confident nothing wrong model fit, can obtain calibration curve setting argument allowPerfectPredictions TRUE. case, predictions 0 1 replaced values 1e-8 1 - 1e-8, respectively. take account interpreting performance measures, calculated original values.","code":"set.seed(1) yTest = testdata$y pHat[sample(1:length(pHat), 5, FALSE)] = sample(0:1, 5, TRUE) x = val.prob.ci.2(pHat, yTest, allowPerfectPredictions = TRUE) #> Warning in val.prob.ci.2(pHat, yTest, allowPerfectPredictions = TRUE): There are predictions with value 0 or 1! These are replaced by values 1e-8 and 1 - 1e-8, respectively. Take this into account when interpreting the performance measures, as these are not calculated with the original values. #>  #> Please check your model, as this may be an indication of overfitting. Predictions of 0 or 1 imply that these predicted values are deterministic. #>  #> We observe this in the following cases: #>  - logistic regression: with quasi-complete separation, the coefficients tend to infinity; #>  - tree-based methods: one of the leaf nodes contains only observations with either 0 or 1; #>  - neural networks: the weights tend to infinity and this is known as weight/gradient explosion."},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"De Cock Bavo. Author, maintainer. Barrenada Lasai. Author. Nieboer Daan. Author. Van Calster Ben. Author. Steyerberg Ewout. Author. Vergouwe Yvonne. Author.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina, M.J., Steyerberg, E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176. doi: 10.1016/j.jclinepi.2015.12.005. De Cock, B., Nieboer, D., Van Calster, B., Steyerberg, E.W., Vergouwe, Y. (2023). CalibrationCurves package: assessing agreement observed outcomes predictions. R package version 2.0.3, doi: 10.32614/CRAN.package.CalibrationCurves, available https://cran.r-project.org/package=CalibrationCurves De Cock Campo, B. (2023). Towards reliable predictive analytics: generalized calibration framework. arXiv: 2309.08559, available https://arxiv.org/abs/2309.08559 Barreñada, L., De Cock Campo, B., Wynants, L., Van Calster, B. (2025). Clustered Flexible Calibration Plots Binary Outcomes Using Random Effects Modeling. arXiv:2503.08389, available https://arxiv.org/abs/2503.08389.","code":"@Article{,   title = {A calibration hierarchy for risk models was defined: from utopia to empirical data},   author = {Ben {Van Calster} and Daan Nieboer and Yvonne Vergouwe and Bavo {De Cock} and Michael J. Pencina and Ewout W. Steyerberg},   journal = {Journal of Clinical Epidemiology},   year = {2016},   volume = {74},   pages = {167--176},   doi = {10.1016/j.jclinepi.2015.12.005}, } @Manual{,   title = {The CalibrationCurves package: assessing the agreement between observed outcomes and predictions.},   author = {Bavo {De Cock} and Daan Nieboer and Ben {Van Calster} and Ewout W. Steyerberg and Yvonne Vergouwe},   year = {2023},   note = {R package version 2.0.3},   doi = {10.32614/CRAN.package.CalibrationCurves},   url = {https://cran.r-project.org/package=CalibrationCurves}, } @Article{,   title = {Towards reliable predictive analytics: a generalized calibration framework},   author = {Bavo {De Cock Campo}},   journal = {arXiv},   year = {2023},   pages = {2309.08559},   url = {https://arxiv.org/abs/2309.08559}, } @Article{,   title = {Clustered Flexible Calibration Plots For Binary Outcomes Using Random Effects Modeling},   author = {Lasai Barreñada and Bavo {De Cock Campo} and Laure Wynants and Ben {Van Calster}},   journal = {arXiv},   year = {2025},   pages = {2503.08389},   url = {https://arxiv.org/abs/2503.08389}, }"},{"path":"https://bavodc.github.io/CalibrationCurves/index.html","id":"calibrationcurves-assessing-the-agreement-between-observed-outcomes-and-predictions","dir":"","previous_headings":"","what":"Calibration Performance","title":"Calibration Performance","text":"Package generate (generalized) calibration curves related statistics. function logistic/flexible calibration curves based val.prob function Frank Harrell’s rms package.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/index.html","id":"on-current-r--300","dir":"","previous_headings":"Installation","what":"On current R (>= 3.0.0)","title":"Calibration Performance","text":"can install latest development version Github using code requires devtools >= 1.6.1, installs “master” branch. approach builds package source.","code":"library(\"devtools\") install_github(\"BavoDC/CalibrationCurves\", dependencies = TRUE, build_vignettes = TRUE, ref = \"master\")"},{"path":"https://bavodc.github.io/CalibrationCurves/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Calibration Performance","text":"basic functionality package explained demonstrated vignette, can access using via homepage package.","code":"vignette(\"CalibrationCurves\")"},{"path":"https://bavodc.github.io/CalibrationCurves/index.html","id":"contact","dir":"","previous_headings":"","what":"Contact","title":"Calibration Performance","text":"questions, remarks suggestions regarding package, can contact bavo.campo@kuleuven.(emails bavo.decock@kuleuven.forwarded one).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Calibration Performance","text":"use package, please cite:  - Barreñada, Lasai, Bavo DC Campo, Laure Wynants, Ben Van Calster. 2025. Clustered Flexible Calibration Plots Binary Outcomes Using Random Effects Modeling. arXiv:2503.08389, available https://arxiv.org/abs/2503.08389.  - De Cock Campo, B. (2023). Towards reliable predictive analytics: generalized calibration framework. arXiv:2309.08559, available https://arxiv.org/abs/2309.08559.  - De Cock, B., Nieboer, D., Van Calster, B., Steyerberg, E.W., Vergouwe, Y. (2023). CalibrationCurves package: assessing agreement observed outcomes predictions. R package version 2.0.3, doi:10.32614/CRAN.package.CalibrationCurves, available https://cran.r-project.org/package=CalibrationCurves  - Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina, M.J., Steyerberg, E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":null,"dir":"Reference","previous_headings":"","what":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"Obtain point estimate confidence interval   AUC various methods based Mann-Whitney statistic.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"","code":"auc.nonpara.mw(x, y, conf.level=0.95,                   method=c(\"newcombe\", \"pepe\", \"delong\",                            \"jackknife\", \"bootstrapP\", \"bootstrapBCa\"),                   nboot)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"x vector observations class P. y vector observations class N. conf.level confidence level interval. default \t0.95. method method used construct CI. newcombe \tmethod recommended Newcombe (2006); pepe method \tproposed Pepe (2003); delong method proposed \tDelong et al. (1988); jackknife uses \tjackknife method; bootstrapP uses bootstrap \tpercentile CI; bootstrapBCa uses bootstrap \tbias-corrected accelerated CI. default newcombe. can abbreviated. nboot number bootstrap iterations.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"function implements various methods based Mann-Whitney statistic.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"observations class P tend larger values class N. help-file copy original help-file function auc.nonpara.mw auRoc-package. important note   , using method=\"pepe\", confidence interval computed documented Qin Hotilovac (2008)   different original function.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"Point estimate lower upper bounds CI AUC.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/auc.nonpara.mw.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"AUC Based on the Mann-Whitney Statistic — auc.nonpara.mw","text":"Elizabeth R Delong, David M Delong, Daniel L Clarke-Pearson (1988)   Comparing areas two correlated receiver operating characteristic curves: nonparametric approach.   Biometrics   44 837-845 Dai Feng, Giuliana Cortese, Richard Baumgartner (2015)   comparison confidence/credible interval methods   area ROC curve continuous diagnostic tests   small sample size.   Statistical Methods Medical Research   DOI: 10.1177/0962280215602040 Robert G Newcombe (2006)   Confidence intervals effect size measure based Mann-Whitney statistic. Part 2: asymptotic methods evaluation.   Statistics medicine   25(4) 559-573 Margaret Sullivan Pepe (2003)   statistical evaluation medical tests classification prediction.   Oxford University Press Qin, G., & Hotilovac, L. (2008). Comparison non-parametric confidence intervals area ROC curve continuous-scale   diagnostic test. Statistical Methods Medical Research, 17(2), pp. 207-21","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CalibrationCurves.html","id":null,"dir":"Reference","previous_headings":"","what":"General information on the package and its functions — CalibrationCurves","title":"General information on the package and its functions — CalibrationCurves","text":"Using package, can assess calibration performance prediction model. , extent predictions correspond observe empirically.  assess calibration model binary outcome, can use val.prob.ci.2 valProbggplot function. outcome prediction model  binary follows different distribution exponential family, can employ genCalCurve function. familiar theory /application calibration, can consult vignette package. vignette provides comprehensive overview theory  contains tutorial practical examples. , suggest reader consult paper generalized calibration curves arXiv.  paper, provide theoretical background generalized calibration framework illustrate applicability prototypical examples statistical  machine learning prediction models well-calibrated, overfit underfit. Originally, package contained functions assess calibration prediction models binary outcome. details section provides background information  history package's development.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CalibrationCurves.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General information on the package and its functions — CalibrationCurves","text":"years ago, Yvonne Vergouwe Ewout Steyerberg adapted function val.prob rms-package (https://cran.r-project.org/package=rms) val.prob.ci added following functions val.prob: Scaled Brier score relating max average calibrated Null model Risk distribution according outcome 0 1 indicate outcome label; set d1lab=\"..\", d0lab=\"..\" Labels: y axis: \"Observed Frequency\"; Triangle: \"Grouped observations\" Confidence intervals around triangles cut-can plotted; set x coordinate December 2015, Bavo De Cock, Daan Nieboer, Ben Van Calster adapted val.prob.ci.2: Flexible calibration curves can obtained using loess (default)     restricted cubic splines, pointwise 95% confidence intervals. Flexible calibration curves now given default decision based findings reported Van Calster et al. (2016). Loess: confidence intervals can obtained closed form using bootstrapping     (CL.BT=T bootstrapping 2000 bootstrap samples, however     take ) RCS: 3 5 knots can used knot locations estimated using default quantiles          x (rcspline.eval, see rcspline.plot rcspline.eval) estimation problems occur specified number knots          (nr.knots, default 5), analysis repeated           nr.knots-1 problem disappeared function stops still estimation problem 3 knots can now adjust plot use normal plot commands     (cex.axis etcetera), size legend now specified     cex.leg Label y-axis: \"Observed proportion\" Stats: added Estimated Calibration Index (ECI), statistical     measure quantify lack calibration (Van Hoorde et al., 2015) Stats shown plot: default show \"abc\" model performance (Steyerberg et al., 2011). , calibration intercept (calibration---large), calibration slope c-     statistic. Alternatively, user can select statistics     choice (e.g. dostats=c(\"C (ROC)\",\"R2\") dostats=c(2,3). Vectors p, y logit longer sorted 2023, Bavo De Cock (Campo) published paper introduces generalized calibration framework. framework extension logistic calibration framework prediction models outcome's distribution member exponential family. , able assess calibration wider range prediction models. methods paper implemented CalibrationCurves package. current version package can always found https://github.com/BavoDC can easily installed using following code: install.packages(\"devtools\") # yet installed require(devtools) install_github(\"BavoDC/CalibrationCurves\", dependencies = TRUE, build_vignettes = TRUE)","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CalibrationCurves.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"General information on the package and its functions — CalibrationCurves","text":"Barreñada, L., De Cock Campo, B., Wynants, L., Van Calster, B. (2025). Clustered Flexible Calibration Plots Binary Outcomes Using Random Effects Modeling. arXiv:2503.08389, available https://arxiv.org/abs/2503.08389. De Cock Campo, B. (2023). Towards reliable predictive analytics: generalized calibration framework. arXiv:2309.08559, available https://arxiv.org/abs/2309.08559. Steyerberg, E.W.Van Calster, B., Pencina, M.J. (2011). Performance measures prediction models markers : evaluation predictions classifications. Revista Espanola de Cardiologia, 64(9), pp. 788-794 Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176 Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). spline-based tool assess visualize calibration multiclass risk predictions. Journal Biomedical Informatics, 54, pp. 283-93","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CGC.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","title":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","text":"Estimates calibration curves using CGC approach. function supports two grouping methods: equal-sized groups (\"grouped\") interval-based groups (\"interval\"). Optionally, calibration plot can produced cluster-specific curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CGC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","text":"","code":"CGC(   data = NULL,   p,   y,   cluster,   cl.level = 0.95,   ntiles = 10,   cluster_curves = FALSE,   plot = TRUE,   size = 1,   linewidth = 0.4,   univariate = FALSE,   method = c(\"grouped\", \"interval\") )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CGC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","text":"data optional data frame containing variables p, y, cluster. supplied, variable names given without quotation marks. p predicted probabilities (numeric vector) name column data. y binary outcome variable name column data. cluster cluster identifier (factor, character, integer) name column data. cl.level confidence level calculation confidence interval. Default 0.95. ntiles integer, number groups (tiles) calibration. Default 10. cluster_curves logical, whether include cluster-specific calibration curves plot. Default FALSE. plot logical, whether return calibration plot. Default TRUE. size numeric, point size plotted curves. Default 1. linewidth numeric, line width plotted curves. Default 0.4. univariate logical, whether use univariate meta-analysis. Default FALSE. method character, grouping method: \"grouped\" (equal-sized groups) \"interval\" (interval-based). Default \"grouped\".","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CGC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","text":"list containing: plot_data Data frame meta-analysis calibration estimates. trad_grouped Data frame traditional grouped calibration results. observed_data Data frame per-observation calibration data. cluster_data Data frame cluster-specific calibration summaries. plot ggplot2 object plot = TRUE, otherwise NULL.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/CGC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function for the Clustered Grouped Calibration Curve (CGC) — CGC","text":"method = \"grouped\", predictions divided equal-sized bins using quantiles. Conversely, method =\"interval\", predictions divided fixed-width bins across [0, 1]. function performs meta-analysis within group. can either univariate bivariate analysis, specified univariate argument. univariate analysis performed using metaprop function bivariate analysis employs rma.mv function. Hereafter, results aggregated plotted calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/clustertraindata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","title":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","text":"clusteredtraindata clusteredtestdata dataframe synthetically generated data sets illustrate functionality package.   clusteredtraindata 1000 observations clusteredtestdata 500 observations. settings used generate data sets.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/clustertraindata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","text":"","code":"data(traindata)   data(testdata)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/clustertraindata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","text":"y binary outcome variable cluster cluster x1 covariate 1 x2 covariate 2 x3 covariate 3 x4 covariate 4 x5 covariate 5","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/clustertraindata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","text":"See examples data sets generated.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/clustertraindata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated data sets to illustrate the package functionality — simulatedclustereddata","text":"","code":"# The data sets were generated as follows   lapply(c(\"magrittr\", \"dplyr\"), library, character.only = TRUE) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:Hmisc': #>  #>     src, summarize #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union #> [[1]] #>  [1] \"magrittr\"          \"CalibrationCurves\" \"ggplot2\"           #>  [4] \"rms\"               \"Hmisc\"             \"stats\"             #>  [7] \"graphics\"          \"grDevices\"         \"utils\"             #> [10] \"datasets\"          \"methods\"           \"base\"              #>  #> [[2]] #>  [1] \"dplyr\"             \"magrittr\"          \"CalibrationCurves\" #>  [4] \"ggplot2\"           \"rms\"               \"Hmisc\"             #>  [7] \"stats\"             \"graphics\"          \"grDevices\"         #> [10] \"utils\"             \"datasets\"          \"methods\"           #> [13] \"base\"              #>    set.seed(1234)    # Simulate training data   nClusters = 10   p         = 5   Uj        = scale(rnorm(nClusters))   nPop      = 1e6   nSample   = 1e3   nTest     = 1e3   X         = replicate(p, rnorm(nPop))   Beta      = rnorm(p)   cluster   = sample(seq_len(nClusters), nPop, TRUE)   table(cluster) #> cluster #>      1      2      3      4      5      6      7      8      9     10  #> 100093 100615 100108 100225 100030  99580  99870  99959  99813  99707    eta       = X %*% Beta + Uj[match(cluster, seq_len(nClusters))]   y         = rbinom(nPop, 1, binomial()$linkinv(eta))   Dt        = data.frame(y, X, cluster)   colnames(Dt) %<>% tolower    clustertraindata = Dt %>%     filter(cluster %in% 1:5) %>%     group_by(cluster) %>%     sample_n(size = nSample) %>%     as.data.frame   clustertestdata = Dt %>%     filter(cluster %in% 6:10) %>%     group_by(cluster) %>%     sample_n(size = nTest) %>%     as.data.frame"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/dot-rcspline.plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function — .rcspline.plot","title":"Internal function — .rcspline.plot","text":"Adjusted version rcspline.plot function output returned plot made","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/dot-rcspline.plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function — .rcspline.plot","text":"","code":".rcspline.plot(   x,   y,   model = c(\"logistic\", \"cox\", \"ols\"),   xrange,   event,   nk = 5,   knots = NULL,   show = c(\"xbeta\", \"prob\"),   adj = NULL,   xlab,   ylab,   ylim,   plim = c(0, 1),   plotcl = TRUE,   showknots = TRUE,   add = FALSE,   plot = TRUE,   subset,   lty = 1,   noprint = FALSE,   m,   smooth = FALSE,   bass = 1,   main = \"auto\",   statloc )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/dot-rcspline.plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function — .rcspline.plot","text":"x numeric predictor y numeric response. binary logistic regression, y either 0 1. model \"logistic\" \"cox\". \"cox\", uses coxph.fit function method=\"efron\" argument set. xrange range evaluating x, default \\(f\\) \\(1 - f\\) quantiles x, \\(f = \\frac{10}{\\max{(n, 200)}}\\) \\(n\\) number observations event event/censoring indicator model=\"cox\". event present, model assumed \"cox\" nk number knots knots knot locations, default based quantiles x (rcspline.eval) show \"xbeta\" \"prob\" - plotted y-axis adj optional matrix adjustment variables xlab x-axis label, default “label” attribute x ylab y-axis label, default “label” attribute y ylim y-axis limits logit log hazard plim y-axis limits probability scale plotcl plot confidence limits showknots show knot locations arrows add add plot already existing plot plot logical indicate whether plot made. FALSE suppresses plot. subset subset observations process, e.g. sex == \"male\" lty line type plotting estimated spline function noprint suppress printing regression coefficients standard errors m model=\"logistic\", plot grouped estimates triangles. group contains m ordered observations x. smooth plot nonparametric estimate model=\"logistic\" adj specified bass smoothing parameter (see supsmu) main main title, default \"Estimated Spline Transformation\" statloc location summary statistics. Default positioning clicking left mouse button upper left corner statistics appear. Alternative \"ll\" place graph lower left, actual x y coordinates. Use \"none\" suppress statistics.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/dot-rcspline.plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function — .rcspline.plot","text":"list components (knots, x, xbeta, lower, upper) respectively knot locations, design matrix, linear predictor, lower upper confidence limits","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration performance using the generalized calibration framework — genCalCurve","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"Function assess calibration performance prediction model outcome's distribution member exponential family (De Cock Campo, 2023). function plots generalized calibration curve computes generalized calibration slope intercept.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"","code":"genCalCurve(   y,   yHat,   family,   plot = TRUE,   Smooth = FALSE,   GLMCal = TRUE,   lwdIdeal = 2,   colIdeal = \"gray\",   ltyIdeal = 1,   lwdSmooth = 1,   colSmooth = \"blue\",   ltySmooth = 1,   argzSmooth = alist(degree = 2),   lwdGLMCal = 1,   colGLMCal = \"red\",   ltyGLMCal = 1,   AddStats = T,   Digits = 3,   cexStats = 1,   lwdLeg = 1.5,   Legend = TRUE,   legendPos = \"bottomright\",   xLim = NULL,   yLim = NULL,   posStats = NULL,   confLimitsSmooth = c(\"none\", \"bootstrap\", \"pointwise\"),   confLevel = 0.95,   Title = \"Calibration plot\",   xlab = \"Predicted value\",   ylab = \"Empirical average\",   EmpiricalDistribution = TRUE,   length.seg = 1,   ... )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"y vector values response variable yHat vector predicted values family description type distribution link function model. can character string naming family function, family function result call family function. (See family details family functions.) plot logical, indicating plot made . Smooth logical, indicating flexible calibration curve estimated. GLMCal logical, indicating GLM calibration curve estimated. lwdIdeal line width ideal line. colIdeal color ideal line. ltyIdeal line type ideal line. lwdSmooth line width flexible calibration curve. colSmooth color flexible calibration curve. ltySmooth line type flexible calibration curve. argzSmooth arguments passed loess. lwdGLMCal line width GLM calibration curve. colGLMCal color GLM calibration curve. ltyGLMCal line type GLM calibration curve. AddStats logical, indicating whether add values generalized calibration slope intercept plot. Digits number digits generalized calibration slope intercept. cexStats font size statistics shown plot. lwdLeg line width legend. Legend logical, indicating whether legend added. legendPos position legend plot. xLim, yLim numeric vectors length 2, giving x y coordinates ranges (see plot.window) posStats numeric vector length 2, specifying x y coordinates statistics (generalized calibration curve intercept) printed plot. Default NULL places statistics top left corner plot. confLimitsSmooth character vector indicate confidence limits flexible calibration curve computed. \"none\" omits confidence limits, \"bootstrap\" uses 2000 bootstrap samples calculate 95% confidence limits \"pointwise\" uses pointwise confidence limits. confLevel confidence level calculation pointwise confidence limits flexible calibration curve. Title title plot xlab x-axis label, default \"Predicted value\". ylab y-axis label, default \"Empirical average\". EmpiricalDistribution logical, indicating empirical distribution predicted values added bottom plot. length.seg controls length histogram lines. Default 1. ... arguments passed plot, see par","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"object type GeneralizedCalibrationCurve following slots: call matched call. ggPlot ggplot object. stats vector containing performance measures calibration. cl.level confidence level used. Calibration contains calibration intercept slope, together confidence intervals. Cindex value c-statistic, together confidence interval. warningMessages , warning messages printed running function. CalibrationCurves coordinates plotting calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"De Cock Campo, B. (2023). Towards reliable predictive analytics: generalized calibration framework. arXiv:2309.08559, available https://arxiv.org/abs/2309.08559.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/genCalCurve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibration performance using the generalized calibration framework — genCalCurve","text":"","code":"library(CalibrationCurves) library(mgcv) #> Loading required package: nlme #>  #> Attaching package: 'nlme' #> The following object is masked from 'package:dplyr': #>  #>     collapse #> This is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'. data(\"poissontraindata\") data(\"poissontestdata\")  glmFit = glm(Y ~ ., data = poissontraindata, family = poisson)  # Example of a well calibrated poisson prediction model yOOS = poissontestdata$Y yHat = predict(glmFit, newdata = poissontestdata, type = \"response\") genCalCurve(yOOS, yHat, family = \"poisson\", plot = TRUE) #> Waiting for profiling to be done... #> Waiting for profiling to be done...  #> Call: #> genCalCurve(y = yOOS, yHat = yHat, family = \"poisson\", plot = TRUE) #>  #> A 95% confidence interval is given for the calibration intercept and calibration slope.  #>  #> Calibration intercept     Calibration slope  #>            0.02710876            0.98320991   # Example of an overfit poisson prediction model gamFit = gam(Y ~ x1 + x3 + x1:x3 + s(x5), data = poissontraindata, family = poisson) yHat = as.vector(predict(gamFit, newdata = poissontestdata, type = \"response\")) genCalCurve(yOOS, yHat, family = \"poisson\", plot = TRUE) #> Waiting for profiling to be done... #> Waiting for profiling to be done...  #> Call: #> genCalCurve(y = yOOS, yHat = yHat, family = \"poisson\", plot = TRUE) #>  #> A 95% confidence interval is given for the calibration intercept and calibration slope.  #>  #> Calibration intercept     Calibration slope  #>           -0.01609734            0.82441039   # Example of an underfit poisson prediction model glmFit = glm(Y ~ x2, data = poissontraindata, family = poisson) yOOS = poissontestdata$Y yHat = predict(glmFit, newdata = poissontestdata, type = \"response\") genCalCurve(yOOS, yHat, family = \"poisson\", plot = TRUE) #> Waiting for profiling to be done... #> Waiting for profiling to be done...  #> Call: #> genCalCurve(y = yOOS, yHat = yHat, family = \"poisson\", plot = TRUE) #>  #> A 95% confidence interval is given for the calibration intercept and calibration slope.  #>  #> Calibration intercept     Calibration slope  #>            0.02454964            1.30572143"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-less-than-equals-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Infix operator to run background jobs — %<=%","title":"Infix operator to run background jobs — %<=%","text":"infix operator can used create background job RStudio/Posit , completed, value rhs assigned lhs.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-less-than-equals-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Infix operator to run background jobs — %<=%","text":"","code":"lhs %<=% rhs"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-less-than-equals-grapes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Infix operator to run background jobs — %<=%","text":"lhs object rhs value assigned rhs value want assign lhs","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-less-than-equals-grapes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Infix operator to run background jobs — %<=%","text":"prints ID background job console , completed, value lhs assigned rhs","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-less-than-equals-grapes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Infix operator to run background jobs — %<=%","text":"","code":"# Can only be executed in Rstudio if (FALSE) x %<=% rnorm(1e7) # \\dontrun{}"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Infix operator to run background jobs — %{}%","title":"Infix operator to run background jobs — %{}%","text":"infix operator can used create background job block code RStudio/Posit , completed, objects created block code imported global environment.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Infix operator to run background jobs — %{}%","text":"","code":"lhs %{}% rhs"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Infix operator to run background jobs — %{}%","text":"lhs used, see details examples rhs block code want run","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Infix operator to run background jobs — %{}%","text":"prints ID background job console , completed, objects created block code imported global environment","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Infix operator to run background jobs — %{}%","text":"can use infix operator two different ways. Either set left-hand side NULL use syntax `%{}%`  ({BlockOfCode})","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/grapes-open-curly-close-grapes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Infix operator to run background jobs — %{}%","text":"","code":"# Can only be executed in Rstudio if (FALSE) { # \\dontrun{ NULL %{}% {  x = rnorm(1e7)  y = rnorm(1e7) } `%{}%` ({  x = rnorm(1e7)  y = rnorm(1e7) }) } # }"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/LibraryM.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to load multiple packages at once — LibraryM","title":"Function to load multiple packages at once — LibraryM","text":"Function load multiple packages ","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/LibraryM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to load multiple packages at once — LibraryM","text":"","code":"LibraryM(...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/LibraryM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to load multiple packages at once — LibraryM","text":"... packages want load","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/LibraryM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to load multiple packages at once — LibraryM","text":"invisible NULL","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/LibraryM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to load multiple packages at once — LibraryM","text":"","code":"LibraryM(CalibrationCurves)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MAC2.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","title":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","text":"Computes meta-analytical calibration curves using multiple methods (logistic regression, loess splines) performs meta-analysis across clusters generate aggregated calibration curves confidence prediction intervals.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MAC2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","text":"","code":"MAC2(   data = NULL,   p,   y,   cluster,   grid,   cl.level = 0.95,   alpha.lr = 0.05/3,   plot = TRUE,   cluster_curves = FALSE,   knots = 3,   transf = \"logit\",   method_choice = c(\"splines\", \"log\", \"loess\"),   method.tau = \"REML\",   prediction = TRUE,   random = TRUE,   sm = \"PLOGIT\",   hakn = FALSE,   linewidth = 1,   method.predict = \"HTS\",   verbose = FALSE )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MAC2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","text":"data optional data frame containing variables p, y, cluster. supplied, variable names given without quotation marks. p predicted probabilities (numeric vector) name column data. y binary outcome variable name column data. cluster Cluster identifier (factor, character, integer) name column data. grid grid calibration curve evaluation cl.level confidence level calculation confidence interval. Default 0.95. alpha.lr alpha-level used likelihood ratio test, selecting number knots restricted cubic splines plot logical, indicates whether plot calibration curves. Default TRUE. cluster_curves logical, whether include cluster-specific curves plot. Default FALSE. knots integer, number knots splines. Default 3. transf character, transformation predictions: \"logit\" \"identity\". Default \"logit\". method_choice character, method use meta-analysis. Options : \"log\", \"loess\" \"splines\". Default \"splines\". method.tau character, method -study heterogeneity estimation. Default \"REML\". argument passed  metagen function. prediction logical, whether compute prediction intervals. Default TRUE. argument passed  prediction argument metagen function. random logical, whether use random-effects model. Default TRUE. argument passed  random argument metagen function. sm character, summary measure meta-analysis. Default \"PLOGIT\". argument passed  sm argument metagen function. hakn logical, whether use Hartung-Knapp adjustment. Default FALSE. argument passed  method.random.ci argument metagen function. linewidth numeric, line width meta-curve. Default 1. method.predict character, method prediction intervals. Default \"HTS\". argument passed  method.predict argument metagen function. verbose logical, indicates whether progress printed console.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MAC2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","text":"list containing: cluster_data Data frame linear predictors standard errors method per cluster plot_data Data frame meta-analysis results including predictions intervals plot ggplot2 object plot = TRUE, otherwise NULL","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MAC2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function for the Meta-Analytical Calibration Curve (MAC2) — MAC2","text":"function estimates center-specific calibration curves using logistic regression, loess splines. Hereafter, aggregates calibration curves using meta-analytical techniques. meta-analysis performed using function metagen meta package. method_choice argument determines method meta-analytical aggregation.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MIXC.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","title":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","text":"Estimates calibration curve using logistic generalized linear mixed model.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MIXC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","text":"","code":"MIXC(   data = NULL,   p,   y,   cluster,   grid,   method = c(\"slope\", \"intercept\"),   plot = TRUE,   cluster_curves = FALSE,   nsims_pi = 10000,   CI = TRUE,   CI_method = c(\"naive\", \"delta\"),   cl.level = 0.95 )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MIXC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","text":"data optional data frame containing variables p, y, cluster. supplied, variable names given without quotation marks. p predicted probabilities (numeric vector) name column data. y binary outcome variable name column data. cluster Cluster identifier (factor, character, integer) name column data. grid grid calibration curve evaluation method character, type mixed-effects model: \"intercept\" (random intercept) \"slope\" (random slope). Default \"slope\". plot logical, indicating whether generate calibration plot. Default TRUE. cluster_curves logical, whether include cluster-specific curves plot. Default FALSE. nsims_pi integer, number simulations prediction intervals. Default 10000. CI logical, whether calculate confidence intervals. Default TRUE. CI_method character, method computing confidence intervals observed proportions. \"delta\", delta method applied. Conversely, CI_method == \"naive\", correction applied. Default \"naive\". cl.level confidence level calculation confidence interval. Default 0.95.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MIXC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","text":"list containing: model fitted mixed-effects model object cluster_data Data frame calibration data cluster plot_data Data frame calibration data average cluster observed_data Data frame calibration data individual observations plot ggplot2 object plot = TRUE, otherwise NULL","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/MIXC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal function for the Mixed-Effects Model Calibration Curve (MIXC) — MIXC","text":"function estimates calibration curves using logistic generalized linear mixed model.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/poissontraindata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","title":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","text":"traindata testdata dataframe synthetically generated data sets illustrate functionality package.   traindata 5000 observations testdata 1000 observations. settings used generate data   sets.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/poissontraindata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","text":"","code":"data(poissontraindata)   data(poissontestdata)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/poissontraindata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","text":"y poisson distributed outcome variable x1 covariate 1 x2 covariate 2 x3 covariate 3 x4 covariate 4 x5 covariate 5","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/poissontraindata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","text":"See examples data sets generated.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/poissontraindata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated data sets to illustrate the package functionality — simulatedpoissondata","text":"","code":"# The data sets were generated as follows   library(MASS) #>  #> Attaching package: 'MASS' #> The following object is masked from 'package:dplyr': #>  #>     select   library(magrittr)   ScaleRange <- function(x, xmin = -1, xmax = 1) {   xRange = range(x)   (x - xRange[1]) / diff(xRange) * (xmax - xmin) + xmin   }    set.seed(144)   p    = 5   N    = 1e6   n    = 5e3   nOOS = 1e3   S    = matrix(NA, 5, 5)   rho  = c(0.025, 0, 0, 0.05, 0.075, 0, 0, 0.025, 0, 0)   S[upper.tri(S)] = rho   S[lower.tri(S)] = t(S)[lower.tri(S)]   diag(S) = 1   Matrix::isSymmetric(S) #> [1] TRUE     X  = mvrnorm(N, rep(0, p), Sigma = S, empirical = TRUE)   X  = apply(X, 2, ScaleRange)   B  = c(-2.3, 1.5, 2, -1, -2, -1.5)   mu = poisson()$linkinv(cbind(1, X) %*% B)   Y  = rpois(N, mu)    Df = data.frame(Y, X)   colnames(Df)[-1] %<>% tolower()    set.seed(2)   DfS   = Df[sample(1:nrow(Df), n, FALSE), ]   DfOOS = Df[sample(1:nrow(Df), nOOS, FALSE), ]    poissontraindata = DfS   poissontestdata  = DfOOS"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.CalibrationCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Print function for a CalibrationCurve object — print.CalibrationCurve","title":"Print function for a CalibrationCurve object — print.CalibrationCurve","text":"Prints call, confidence level values performance measures.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.CalibrationCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print function for a CalibrationCurve object — print.CalibrationCurve","text":"","code":"# S3 method for class 'CalibrationCurve' print(x, ...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.CalibrationCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print function for a CalibrationCurve object — print.CalibrationCurve","text":"x object type CalibrationCurve, resulting val.prob.ci.2. ... arguments passed print","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.CalibrationCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print function for a CalibrationCurve object — print.CalibrationCurve","text":"original CalibrationCurve object returned.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ClusteredCalibrationCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Print function for a ClusteredCalibrationCurve object — print.ClusteredCalibrationCurve","title":"Print function for a ClusteredCalibrationCurve object — print.ClusteredCalibrationCurve","text":"Prints ggplot, call, confidence level values performance measures.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ClusteredCalibrationCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print function for a ClusteredCalibrationCurve object — print.ClusteredCalibrationCurve","text":"","code":"# S3 method for class 'ClusteredCalibrationCurve' print(x, ...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ClusteredCalibrationCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print function for a ClusteredCalibrationCurve object — print.ClusteredCalibrationCurve","text":"x object type ggplotCalibrationCurve, resulting valProbggplot. ... arguments passed print","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ClusteredCalibrationCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print function for a ClusteredCalibrationCurve object — print.ClusteredCalibrationCurve","text":"original ggplotCalibrationCurve object returned.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.GeneralizedCalibrationCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Print function for a GeneralizedCalibrationCurve object — print.GeneralizedCalibrationCurve","title":"Print function for a GeneralizedCalibrationCurve object — print.GeneralizedCalibrationCurve","text":"Prints call, confidence level values performance measures.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.GeneralizedCalibrationCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print function for a GeneralizedCalibrationCurve object — print.GeneralizedCalibrationCurve","text":"","code":"# S3 method for class 'GeneralizedCalibrationCurve' print(x, ...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.GeneralizedCalibrationCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print function for a GeneralizedCalibrationCurve object — print.GeneralizedCalibrationCurve","text":"x object type GeneralizedCalibrationCurve, resulting genCalCurve. ... arguments passed print","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.GeneralizedCalibrationCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print function for a GeneralizedCalibrationCurve object — print.GeneralizedCalibrationCurve","text":"original GeneralizedCalibrationCurve object returned.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ggplotCalibrationCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Print function for a ggplotCalibrationCurve object — print.ggplotCalibrationCurve","title":"Print function for a ggplotCalibrationCurve object — print.ggplotCalibrationCurve","text":"Prints ggplot, call, confidence level values performance measures.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ggplotCalibrationCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print function for a ggplotCalibrationCurve object — print.ggplotCalibrationCurve","text":"","code":"# S3 method for class 'ggplotCalibrationCurve' print(x, ...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ggplotCalibrationCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print function for a ggplotCalibrationCurve object — print.ggplotCalibrationCurve","text":"x object type ggplotCalibrationCurve, resulting valProbggplot. ... arguments passed print","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.ggplotCalibrationCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print function for a ggplotCalibrationCurve object — print.ggplotCalibrationCurve","text":"original ggplotCalibrationCurve object returned.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.SurvivalCalibrationCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"Print function for a SurvivalCalibrationCurve object — print.SurvivalCalibrationCurve","title":"Print function for a SurvivalCalibrationCurve object — print.SurvivalCalibrationCurve","text":"Print function SurvivalCalibrationCurve object","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.SurvivalCalibrationCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print function for a SurvivalCalibrationCurve object — print.SurvivalCalibrationCurve","text":"","code":"# S3 method for class 'SurvivalCalibrationCurve' print(x, ...)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.SurvivalCalibrationCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print function for a SurvivalCalibrationCurve object — print.SurvivalCalibrationCurve","text":"x object type SurvivalCalibrationCurve, resulting valProbSurvival. ... arguments passed print","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/print.SurvivalCalibrationCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print function for a SurvivalCalibrationCurve object — print.SurvivalCalibrationCurve","text":"original SurvivalCalibrationCurve object returned.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":null,"dir":"Reference","previous_headings":"","what":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"training dataset contains real-life survival data patients underwent primary surgery breast cancer 1978 1993 Rotterdam. patients followed 2007, resulting model development cohort 2982 patients exclusions. primary outcome measured recurrence-free survival, defined time primary surgery recurrence death. validation dataset consists 686 patients primary node-positive breast cancer German Breast Cancer Study Group. cohort, 285 patients suffered recurrence died within 5 years follow-, 280 censored 5 years. Five-year predictions chosen lowest median survival two cohorts (Rotterdam cohort, 6.7 years; German cohort, 4.9 years).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"","code":"data(trainDataSurvival)   data(testDataSurvival)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"data frame observations following 26 variables. pid patient identifier year year surgery age age surgery meno menopausal status (0 = premenopausal, 1 = postmenopausal) size tumor size, factor levels <= 20, 20-50, >50 grade differentiation grade nodes number positive lymph nodes pgr progesterone receptors (fmol/l) er estrogen receptors (fmol/l) hormon hormonal treatment (0 = , 1 = yes) chemo chemotherapy rtime days relapse last follow-recur 0 = relapse, 1 = relapse dtime days death last follow-death 0 = alive, 1 = dead ryear Follow-time RFS, years (numeric) rfs Recurrence-free survival status (0 = event, 1 = event) (numeric) pgr2 Winsorized progesterone receptor level (numeric) nodes2 Winsorized node count (numeric) csize Categorized tumor size, copied size (factor) cnode Categorized node involvement (factor: \"0\", \"1-3\", \">3\") grade3 Recoded grade factor (levels: \"1-2\", \"3\") nodes3 Restricted cubic spline basis nodes2 (numeric) pgr3 Restricted cubic spline basis original pgr (numeric) epoch Follow-epoch indicator splitting 5 years (numeric)","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"data sets based publicly available code data used repository Prediction_performance_survival Giardiello et al. (2023), accompanies Annals Internal Medicine article \"Assessing Performance Clinical Usefulness Prediction Models Survival Outcomes: Practical Guidance Cox Proportional Hazards Models\". preprocessing steps, converting survival time years, defining recurrence-free survival status via `rfs = pmax(recur, death)`, correcting 43 discordant cases using death time, 99th-percentile winsorization `pgr` `nodes`, spline transformations (`nodes3`, `pgr3`), splitting follow-5 years (`epoch`), recoding categorical variables (`csize`, `cnode`, `grade3`)—performed exactly Giardiello code. training dataset, trainDataSurvival, consists 2982 patients, 1713 events occurring maximum follow-time 19.3 years. estimated median potential follow-time, calculated using reverse Kaplan- method, 9.3 years. patients, 1275 suffered recurrence death within follow-time interest (5 years), 126 censored 5 years. validation dataset, testDataSurvival, consists 686 patients primary node-positive breast cancer German Breast Cancer Study Group. cohort, 285 patients suffered recurrence died within 5 years follow-, 280 censored 5 years. Five-year predictions chosen lowest median survival two cohorts (Rotterdam cohort, 6.7 years; German cohort, 4.9 years).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"David J. McLernon, Daniele Giardiello, Ben Van Calster, et al. (2023). Assessing Performance Clinical Usefulness Prediction Models Survival Outcomes: Practical Guidance Cox Proportional Hazards Models. Annals Internal Medicine, 176(1), pp. 105-114, doi:10.7326/M22-0844","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/survivaltraindata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Breast Cancer Survival Data from Rotterdam and Germany — simulatedsurvivaldata","text":"","code":"data(testDataSurvival) ## Explore the structure of the dataset str(testDataSurvival) #> 'data.frame':\t686 obs. of  21 variables: #>  $ pid    : int  132 1575 1140 769 130 1642 475 973 569 1180 ... #>  $ age    : int  49 55 56 45 65 48 48 37 67 45 ... #>  $ meno   : int  0 1 1 0 1 0 0 0 1 0 ... #>  $ size   : int  18 20 40 25 30 52 21 20 20 30 ... #>  $ grade  : int  2 3 3 3 2 2 3 2 2 2 ... #>  $ nodes  : int  2 16 3 1 5 11 8 9 1 1 ... #>  $ pgr    : int  0 0 0 0 0 0 0 0 0 0 ... #>  $ er     : int  0 0 0 4 36 0 0 0 0 0 ... #>  $ hormon : int  0 0 0 0 1 0 0 1 1 0 ... #>  $ rfstime: int  1838 403 1603 177 1855 842 293 42 564 1093 ... #>  $ status : int  0 1 0 0 0 1 1 0 1 1 ... #>  $ cnode  : Factor w/ 3 levels \"0\",\"1-3\",\">3\": 2 3 2 2 3 3 3 3 2 2 ... #>  $ csize  : Factor w/ 3 levels \"<=20\",\"20-50\",..: 1 1 2 2 2 3 2 1 1 2 ... #>  $ pgr2   : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ nodes2 : num  2 16 3 1 5 11 8 9 1 1 ... #>  $ grade3 : Factor w/ 2 levels \"1-2\",\"3\": 1 2 2 2 1 1 2 1 1 1 ... #>  $ nodes3 : num  0.0849 4.2222 0.2222 0.0123 0.6543 ... #>  $ pgr3   : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ ryear  : num  5 1.103 4.389 0.485 5 ... #>  $ rfs    : num  0 1 0 0 0 1 1 0 1 1 ... #>  $ epoch  : num  1 1 1 1 1 1 1 1 1 1 ..."},{"path":"https://bavodc.github.io/CalibrationCurves/reference/traindata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data sets to illustrate the package functionality — simulateddata","title":"Simulated data sets to illustrate the package functionality — simulateddata","text":"traindata testdata dataframe synthetically generated data sets illustrate functionality package.   traindata 1000 observations testdata 500 observations. settings used generate data   sets.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/traindata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated data sets to illustrate the package functionality — simulateddata","text":"","code":"data(traindata)   data(testdata)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/traindata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data sets to illustrate the package functionality — simulateddata","text":"y binary outcome variable x1 covariate 1 x2 covariate 2 x3 covariate 3 x4 covariate 4","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/traindata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulated data sets to illustrate the package functionality — simulateddata","text":"See examples data sets generated.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/traindata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated data sets to illustrate the package functionality — simulateddata","text":"","code":"# The data sets were generated as follows   set.seed(1782)    # Simulate training data   nTrain    = 1000   B         = c(0.1, 0.5, 1.2, -0.75, 0.8)   X         = replicate(4, rnorm(nTrain))   p0true    = binomial()$linkinv(cbind(1, X) %*% B)   y         = rbinom(nTrain, 1, p0true)   colnames(X) = paste0(\"x\", seq_len(ncol(X)))   traindata = data.frame(y, X)    # Simulate validation data   nTest    = 500   X        = replicate(4, rnorm(nTest))   p0true   = binomial()$linkinv(cbind(1, X) %*% B)   y        = rbinom(nTest, 1, p0true)   colnames(X) = paste0(\"x\", seq_len(ncol(X)))   testdata = data.frame(y, X)"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration performance — val.prob.ci.2","title":"Calibration performance — val.prob.ci.2","text":"function val.prob.ci.2 adaptation val.prob Frank Harrell's rms package, https://cran.r-project.org/package=rms. Hence, description functions val.prob.ci.2 come original val.prob.  key feature val.prob.ci.2 generation logistic flexible calibration curves related statistics. using code, please cite: Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina, M.J., Steyerberg, E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration performance — val.prob.ci.2","text":"","code":"val.prob.ci.2(   p,   y,   logit,   group,   weights = rep(1, length(y)),   normwt = FALSE,   pl = TRUE,   smooth = c(\"loess\", \"rcs\", \"none\"),   CL.smooth = \"fill\",   CL.BT = FALSE,   lty.smooth = 1,   col.smooth = \"black\",   lwd.smooth = 1,   nr.knots = 5,   logistic.cal = FALSE,   lty.log = 1,   col.log = \"black\",   lwd.log = 1,   xlab = \"Predicted probability\",   ylab = \"Observed proportion\",   xlim = c(-0.02, 1),   ylim = c(-0.15, 1),   m,   g,   cuts,   emax.lim = c(0, 1),   legendloc = c(0.5, 0.27),   statloc = c(0, 0.85),   dostats = TRUE,   cl.level = 0.95,   method.ci = \"pepe\",   roundstats = 2,   riskdist = \"predicted\",   cex = 0.75,   cex.leg = 0.75,   connect.group = FALSE,   connect.smooth = TRUE,   g.group = 4,   evaluate = 100,   nmin = 0,   d0lab = \"0\",   d1lab = \"1\",   cex.d01 = 0.7,   dist.label = 0.04,   line.bins = -0.05,   dist.label2 = 0.03,   cutoff,   las = 1,   length.seg = 1,   y.intersp = 1,   lty.ideal = 1,   col.ideal = \"red\",   lwd.ideal = 1,   allowPerfectPredictions = FALSE,   argzLoess = alist(degree = 2),   ... )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibration performance — val.prob.ci.2","text":"p predicted probability y vector binary outcomes logit predicted log odds outcome.  Specify either p logit. group grouping variable.  numeric variable grouped g.group quantile groups (default quartiles).  Set group=TRUE use group algorithm single stratum val.prob. weights optional numeric vector per-observation weights (usually frequencies), used group given. normwt set TRUE make weights sum number non-missing observations. pl TRUE plot calibration curve(s). FALSE calibration curves plotted, statistics still computed outputted. smooth \"loess\" generates flexible calibration curve based loess, \"rcs\" generates calibration curves based restricted cubic splines (see rcs rcspline.plot), \"none\" suppresses flexible curve. recommend use loess unless N large,  example N>5000. Default \"loess\". CL.smooth \"fill\" shows pointwise 95% confidence limits flexible calibration curve gray area lower upper limits, TRUE shows pointwise 95% confidence limits flexible calibration curve  dashed lines, FALSE suppresses confidence limits. Default \"fill\". CL.BT TRUE uses confidence limits based 2000 bootstrap samples, FALSE uses closed form confidence limits. Default FALSE. lty.smooth linetype flexible calibration curve. Default 1. col.smooth color flexible calibration curve. Default \"black\". lwd.smooth line width flexible calibration curve. Default 1. nr.knots specifies number knots rcs-based calibration curve. default well highest allowed value 5. case specified number knots leads estimation problems, number knots automatically reduced closest  value without estimation problems. logistic.cal TRUE plots logistic calibration curve, FALSE suppresses curve. Default FALSE. lty.log logistic.cal=TRUE, linetype logistic calibration curve. Default 1. col.log logistic.cal=TRUE, color logistic calibration curve. Default \"black\". lwd.log logistic.cal=TRUE, line width logistic calibration curve. Default 1. xlab x-axis label, default \"Predicted Probability\". ylab y-axis label, default \"Observed proportion\". xlim, ylim numeric vectors length 2, giving x y coordinates ranges (see plot.window) m grouped proportions desired, minimum . observations per group g grouped proportions desired, number quantile groups cuts grouped proportions desired, actual cut points constructing intervals, e.g. c(0,.1,.8,.9,1) seq(0,1,=.2) emax.lim Vector containing lowest highest predicted probability compute Emax. legendloc pl=TRUE, list components x,y vector c(x,y) bottom right corner legend curves points. Default c(.50, .27) scaled lim. Use locator(1) use mouse, FALSE suppress legend. statloc \"abc\" model performance (Steyerberg et al., 2011)-calibration intercept, calibration slope, c statistic-added plot, using statloc upper left corner box (default c(0,.85). can specify list vector. Use locator(1) mouse, FALSE suppress statistics. plotted curve legends. dostats specifies whether performance measures shown figure. TRUE shows \"abc\" model performance (Steyerberg et al., 2011): calibration intercept, calibration slope,  c-statistic. TRUE default.  FALSE suppresses presentation statistics figure. c() list specific stats shows specified  stats. key stats also mentioned paper \"C (ROC)\" c statistic, \"Intercept\"  calibration intercept, \"Slope\" calibration slope, \"ECI\" estimated calibration index  (Van Hoorde et al, 2015). full list possible statistics taken val.prob  augmented estimated calibration index: \"Dxy\", \"C (ROC)\", \"R2\", \"D\", \"D:Chi-sq\", \"D:p\", \"U\", \"U:Chi-sq\",   \"U:p\", \"Q\", \"Brier\", \"Intercept\", \"Slope\", \"Emax\", \"Brier scaled\", \"Eavg\", \"ECI\". statistics always returned function. cl.level dostats=TRUE, confidence level calculation confidence intervals calibration intercept, calibration slope c-statistic. Default 0.95. method.ci method calculate confidence interval c-statistic. argument passed auc.nonpara.mw auRoc-package possible methods compute confidence interval \"newcombe\", \"pepe\", \"delong\" \"jackknife\". Bootstrap-based methods available. default method \"pepe\" , confidence interval logit-transformation-based confidence interval documented Qin Hotilovac (2008). See auc.nonpara.mw information methods. roundstats specifies number decimals statistics rounded shown plot. Default 2. riskdist Use \"calibrated\" plot relative frequency distribution calibrated probabilities dividing 101 bins lim[1] lim[2]. Set \"predicted\" (default rms 4.5-1) use raw assigned risk, FALSE omit risk distribution. Values scaled highest bar 0.15*(lim[2]-lim[1]). cex, cex.leg controls font size statistics (cex) plot legend (cex.leg). Default 0.75 connect.group Defaults FALSE represent group fractions triangles. Set TRUE also connect solid line. connect.smooth Defaults TRUE draw smoothed estimates using line. Set FALSE instead use dots individual estimates g.group number quantile groups use group given variable numeric. evaluate number points store lowess-calibration curve. Default 100.  evaluate unique predicted probabilities, evaluate equally-spaced quantiles unique predicted probabilities, linearly interpolated calibrated values, retained plotting (stored object returned val.prob. nmin applies group given.  nmin \\(> 0\\), val.prob store coordinates smoothed calibration curves outer tails, fewer nmin raw observations represented tails.  example nmin=50, plot function plot estimated calibration curve \\(\\) \\(b\\), 50 subjects predicted probabilities \\(< \\) \\(> b\\). nmin ignored computing accuracy statistics. d0lab, d1lab controls labels events non-events (.e. outcome y) histograms. Defaults d1lab=\"1\" events d0lab=\"0\" non-events. cex.d01 controls size labels events non-events. Default 0.7. dist.label controls horizontal position labels events non-events. Default 0.04. line.bins controls horizontal (y-axis) position histograms. Default -0.05. dist.label2 controls vertical distance labels events non-events. Default 0.03. cutoff puts arrow specified risk cut-(s). Default none. las controls whether y-axis values shown horizontally (1) vertically (0). length.seg controls length histogram lines. Default 1. y.intersp character interspacing vertical line distances legend (legend) lty.ideal linetype ideal line. Default 1. col.ideal controls color ideal line plot. Default \"red\". lwd.ideal controls line width ideal line plot. Default 1. allowPerfectPredictions Logical, indicates whether perfect predictions (.e. values either 0 1) allowed. Default FALSE, since transform predictions using logit transformation calculate calibration measures. case 0 1, results minus infinity infinity, respectively. allowPerfectPredictions = TRUE, 0 1 replaced 1e-8 1 - 1e-8, respectively. argzLoess list arguments passed loess function ... arguments passed plot, see par","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibration performance — val.prob.ci.2","text":"object type CalibrationCurve following slots: call matched call. stats vector containing performance measures calibration. cl.level confidence level used. Calibration contains calibration intercept slope, together confidence intervals. Cindex value c-statistic, together confidence interval. warningMessages , warning messages printed running function. CalibrationCurves coordinates plotting calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calibration performance — val.prob.ci.2","text":"using predicted probabilities uninformative model (.e. equal probabilities observations), model predictive value.  Consequently, applicable, value performance measure corresponds worst possible theoretical value. ECI, example, equals 1 (Edlinger et al., 2022).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calibration performance — val.prob.ci.2","text":"order make use (functions) package auRoc, user needs install JAGS. However, since package uses auc.nonpara.mw function depend use JAGS, therefore copied code slightly adjusted method=\"pepe\".","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibration performance — val.prob.ci.2","text":"Edlinger, M, van Smeden, M, Alber, HF, Wanitschek, M, Van Calster, B. (2022). Risk prediction models discrete ordinal outcomes: Calibration impact proportional odds assumption. Statistics Medicine, 41( 8), pp. 1334– 1360 Qin, G., & Hotilovac, L. (2008). Comparison non-parametric confidence intervals area ROC curve continuous-scale diagnostic test. Statistical Methods Medical Research, 17(2), pp. 207-21 Steyerberg, E.W., Van Calster, B., Pencina, M.J. (2011). Performance measures prediction models markers : evaluation predictions classifications. Revista Espanola de Cardiologia, 64(9), pp. 788-794 Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176 Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). spline-based tool assess visualize calibration multiclass risk predictions. Journal Biomedical Informatics, 54, pp. 283-93","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/val.prob.ci.2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibration performance — val.prob.ci.2","text":"","code":"# Load package library(CalibrationCurves) set.seed(1783)  # Simulate training data X      = replicate(4, rnorm(5e2)) p0true = binomial()$linkinv(cbind(1, X) %*% c(0.1, 0.5, 1.2, -0.75, 0.8)) y      = rbinom(5e2, 1, p0true) Df     = data.frame(y, X)  # Fit logistic model FitLog = lrm(y ~ ., Df)  # Simulate validation data Xval   = replicate(4, rnorm(5e2)) p0true = binomial()$linkinv(cbind(1, Xval) %*% c(0.1, 0.5, 1.2, -0.75, 0.8)) yval   = rbinom(5e2, 1, p0true) Pred   = binomial()$linkinv(cbind(1, Xval) %*% coef(FitLog))  # Default calibration plot val.prob.ci.2(Pred, yval)  #> Call: #> val.prob.ci.2(p = Pred, y = yval) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083   # Adding logistic calibration curves and other additional features val.prob.ci.2(Pred, yval, CL.smooth = TRUE, logistic.cal = TRUE, lty.log = 2,  col.log = \"red\", lwd.log = 1.5)  #> Call: #> val.prob.ci.2(p = Pred, y = yval, CL.smooth = TRUE, logistic.cal = TRUE,  #>     lty.log = 2, col.log = \"red\", lwd.log = 1.5) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083   val.prob.ci.2(Pred, yval, CL.smooth = TRUE, logistic.cal = TRUE, lty.log = 9, col.log = \"red\", lwd.log = 1.5, col.ideal = colors()[10], lwd.ideal = 0.5)  #> Call: #> val.prob.ci.2(p = Pred, y = yval, CL.smooth = TRUE, logistic.cal = TRUE,  #>     lty.log = 9, col.log = \"red\", lwd.log = 1.5, col.ideal = colors()[10],  #>     lwd.ideal = 0.5) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"function evaluates calibration performance model's predicted probabilities whilst accounting clustering. function supports multiple approaches (`\"CGC\"`, `\"MAC2\"`, `\"MIXC\"`) returns results well `ggplot` object.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"","code":"valProbCluster(   data = NULL,   p,   y,   cluster,   plot = TRUE,   approach = c(\"MIXC\", \"CGC\", \"MAC2\"),   cl.level = 0.95,   xlab = \"Predicted probability\",   ylab = \"Observed proportion\",   grid_l = 100,   rangeGrid = range(p),   ... )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"data optional, data frame containing variables p, y, cluster. supplied, variable names given without quotation marks. p predicted probabilities (numeric vector) name column data y binary outcome variable name column data cluster cluster identifier (factor, character, integer) name column data plot logical, indicates whether plot needs produced. TRUE, plot constructed chosen subfunction. approach character string specifying calibration method use. Must one following: \"CGC\": Clustered Grouped Calibration; \"MAC2\": Meta-Analytical Calibration Curve; \"MIXC\": Mixed-Effects Model Calibration. Defaults \"MIXC\". cl.level confidence level calculation confidence intervals. Default 0.95. xlab label x-axis plot (default \"Predicted probability\"). ylab label y-axis plot (default \"Observed proportion\"). grid_l integer. Number points probability grid plotting (default 100). rangeGrid range grid. Default range(p). ... additional arguments passed selected subfunction (CGC, MAC2 MIXC).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"object class \"valProbCluster\" containing: call: matched call. approach: chosen approach. cl.level: confidence level used. grid: probability grid used plotting. ggplot: ggplot object returned subfunction,         otherwise NULL. results: results chosen subfunction.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"function internally calls one following subfunctions: CGC(p, y, cluster, plot, ...) MAC2(p, y, cluster, plot, grid, ...) MIXC(p, y, cluster, plot, CI, grid, ...) Extra arguments supplied via ellipsis argument ... passed directly chosen subfunction. Please check additional documentation CGC, MAC2 MIXC detailed information arguments.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"Barreñada, L., De Cock Campo, B., Wynants, L., Van Calster, B. (2025). Clustered Flexible Calibration Plots Binary Outcomes Using Random Effects Modeling. arXiv:2503.08389, available https://arxiv.org/abs/2503.08389.","code":""},{"path":[]},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbCluster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibration performance with cluster adjustment (ggplot version) — valProbCluster","text":"","code":"# \\donttest{ library(lme4) #> Loading required package: Matrix #>  #> Attaching package: 'lme4' #> The following object is masked from 'package:nlme': #>  #>     lmList data(\"clustertraindata\") data(\"clustertestdata\") mFit = glmer(y ~ x1 + x2 + x3 + x5 + (1 | cluster),              data = clustertraindata, family = \"binomial\") preds          = predict(mFit, clustertestdata, type = \"response\", re.form = NA) y              = clustertestdata$y cluster        = clustertestdata$cluster valClusterData = data.frame(y = y, preds = preds, center = cluster)  # Assess calibration performance Results  = valProbCluster( p = valClusterData$preds, y = valClusterData$y, cluster = valClusterData$center, plot = TRUE, approach = \"MIXC\", method = \"slope\", grid_l = 100 ) Results  #> Call: #> valProbCluster(p = valClusterData$preds, y = valClusterData$y,  #>     cluster = valClusterData$center, plot = TRUE, approach = \"MIXC\",  #>     grid_l = 100, method = \"slope\") #>  #> A 95% confidence interval is used.  #>  # }"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration performance: ggplot version — valProbggplot","title":"Calibration performance: ggplot version — valProbggplot","text":"function valProbggplot adaptation val.prob Frank Harrell's rms package, https://cran.r-project.org/package=rms. Hence, description functions valProbggplot come original val.prob.  key feature valProbggplot generation logistic flexible calibration curves related statistics. using code, please cite: Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina, M.J., Steyerberg, E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration performance: ggplot version — valProbggplot","text":"","code":"valProbggplot(   p,   y,   logit,   group,   weights = rep(1, length(y)),   normwt = FALSE,   pl = TRUE,   smooth = c(\"loess\", \"rcs\", \"none\"),   CL.smooth = \"fill\",   CL.BT = FALSE,   lty.smooth = 1,   col.smooth = \"black\",   lwd.smooth = 1,   nr.knots = 5,   logistic.cal = FALSE,   lty.log = 1,   col.log = \"black\",   lwd.log = 1,   xlab = \"Predicted probability\",   ylab = \"Observed proportion\",   xlim = c(-0.02, 1),   ylim = c(-0.15, 1),   m,   g,   cuts,   emax.lim = c(0, 1),   legendloc = c(0.5, 0.27),   statloc = c(0, 0.85),   dostats = TRUE,   cl.level = 0.95,   method.ci = \"pepe\",   roundstats = 2,   riskdist = \"predicted\",   size = 3,   size.leg = 5,   connect.group = FALSE,   connect.smooth = TRUE,   g.group = 4,   evaluate = 100,   nmin = 0,   d0lab = \"0\",   d1lab = \"1\",   size.d01 = 5,   dist.label = 0.01,   line.bins = -0.05,   dist.label2 = 0.04,   cutoff,   length.seg = 0.85,   lty.ideal = 1,   col.ideal = \"red\",   lwd.ideal = 1,   allowPerfectPredictions = FALSE,   argzLoess = alist(degree = 2) )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibration performance: ggplot version — valProbggplot","text":"p predicted probability y vector binary outcomes logit predicted log odds outcome.  Specify either p logit. group grouping variable.  numeric variable grouped g.group quantile groups (default quartiles).  Set group=TRUE use group algorithm single stratum val.prob. weights optional numeric vector per-observation weights (usually frequencies), used group given. normwt set TRUE make weights sum number non-missing observations. pl TRUE plot calibration curve(s). FALSE calibration curves plotted, statistics still computed outputted. smooth \"loess\" generates flexible calibration curve based loess, \"rcs\" generates calibration curves based restricted cubic splines (see rcs rcspline.plot), \"none\" suppresses flexible curve. recommend use loess unless N large,  example N>5000. Default \"loess\". CL.smooth \"fill\" shows pointwise 95% confidence limits flexible calibration curve gray area lower upper limits, TRUE shows pointwise 95% confidence limits flexible calibration curve  dashed lines, FALSE suppresses confidence limits. Default \"fill\". CL.BT TRUE uses confidence limits based 2000 bootstrap samples, FALSE uses closed form confidence limits. Default FALSE. lty.smooth linetype flexible calibration curve. Default 1. col.smooth color flexible calibration curve. Default \"black\". lwd.smooth line width flexible calibration curve. Default 1. nr.knots specifies number knots rcs-based calibration curve. default well highest allowed value 5. case specified number knots leads estimation problems, number knots automatically reduced closest  value without estimation problems. logistic.cal TRUE plots logistic calibration curve, FALSE suppresses curve. Default FALSE. lty.log logistic.cal=TRUE, linetype logistic calibration curve. Default 1. col.log logistic.cal=TRUE, color logistic calibration curve. Default \"black\". lwd.log logistic.cal=TRUE, line width logistic calibration curve. Default 1. xlab x-axis label, default \"Predicted Probability\". ylab y-axis label, default \"Observed proportion\". xlim, ylim numeric vectors length 2, giving x y coordinates ranges (see xlim ylim). m grouped proportions desired, minimum . observations per group g grouped proportions desired, number quantile groups cuts grouped proportions desired, actual cut points constructing intervals, e.g. c(0,.1,.8,.9,1) seq(0,1,=.2) emax.lim Vector containing lowest highest predicted probability compute Emax. legendloc pl=TRUE, list components x,y vector c(x,y) bottom right corner legend curves points. Default c(.50, .27) scaled lim. Use locator(1) use mouse, FALSE suppress legend. statloc \"abc\" model performance (Steyerberg et al., 2011)-calibration intercept, calibration slope, c statistic-added plot, using statloc upper left corner box (default c(0,.85). can specify list vector. Use locator(1) mouse, FALSE suppress statistics. plotted curve legends. dostats specifies whether performance measures shown figure. TRUE shows \"abc\" model performance (Steyerberg et al., 2011): calibration intercept, calibration slope,  c-statistic. TRUE default.  FALSE suppresses presentation statistics figure. c() list specific stats shows specified  stats. key stats also mentioned paper \"C (ROC)\" c statistic, \"Intercept\"  calibration intercept, \"Slope\" calibration slope, \"ECI\" estimated calibration index  (Van Hoorde et al, 2015). full list possible statistics taken val.prob  augmented estimated calibration index: \"Dxy\", \"C (ROC)\", \"R2\", \"D\", \"D:Chi-sq\", \"D:p\", \"U\", \"U:Chi-sq\",   \"U:p\", \"Q\", \"Brier\", \"Intercept\", \"Slope\", \"Emax\", \"Brier scaled\", \"Eavg\", \"ECI\". statistics always returned function. cl.level dostats=TRUE, confidence level calculation confidence intervals calibration intercept, calibration slope c-statistic. Default 0.95. method.ci method calculate confidence interval c-statistic. argument passed auc.nonpara.mw auRoc-package possible methods compute confidence interval \"newcombe\", \"pepe\", \"delong\" \"jackknife\". Bootstrap-based methods available. default method \"pepe\" , confidence interval logit-transformation-based confidence interval documented Qin Hotilovac (2008). See auc.nonpara.mw information methods. roundstats specifies number decimals statistics rounded shown plot. Default 2. riskdist Use \"calibrated\" plot relative frequency distribution calibrated probabilities dividing 101 bins lim[1] lim[2]. Set \"predicted\" (default rms 4.5-1) use raw assigned risk, FALSE omit risk distribution. Values scaled highest bar 0.15*(lim[2]-lim[1]). size, size.leg controls font size statistics (size) plot legend (size.leg). Default 3 5, respectively. connect.group Defaults FALSE represent group fractions triangles. Set TRUE also connect solid line. connect.smooth Defaults TRUE draw smoothed estimates using line. Set FALSE instead use dots individual estimates g.group number quantile groups use group given variable numeric. evaluate number points store lowess-calibration curve. Default 100.  evaluate unique predicted probabilities, evaluate equally-spaced quantiles unique predicted probabilities, linearly interpolated calibrated values, retained plotting (stored object returned val.prob. nmin applies group given.  nmin \\(> 0\\), val.prob store coordinates smoothed calibration curves outer tails, fewer nmin raw observations represented tails.  example nmin=50, plot function plot estimated calibration curve \\(\\) \\(b\\), 50 subjects predicted probabilities \\(< \\) \\(> b\\). nmin ignored computing accuracy statistics. d0lab, d1lab controls labels events non-events (.e. outcome y) histograms. Defaults d1lab=\"1\" events d0lab=\"0\" non-events. size.d01 controls size labels events non-events. Default 5. dist.label controls horizontal position labels events non-events. Default 0.01. line.bins controls horizontal (y-axis) position histograms. Default -0.05. dist.label2 controls vertical distance labels events non-events. Default 0.03. cutoff puts arrow specified risk cut-(s). Default none. length.seg controls length histogram lines. Default 0.85. lty.ideal linetype ideal line. Default 1. col.ideal controls color ideal line plot. Default \"red\". lwd.ideal controls line width ideal line plot. Default 1. allowPerfectPredictions Logical, indicates whether perfect predictions (.e. values either 0 1) allowed. Default FALSE, since transform predictions using logit transformation calculate calibration measures. case 0 1, results minus infinity infinity, respectively. allowPerfectPredictions = TRUE, 0 1 replaced 1e-8 1 - 1e-8, respectively. argzLoess list arguments passed loess function","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibration performance: ggplot version — valProbggplot","text":"object type ggplotCalibrationCurve following slots: call matched call. ggPlot ggplot object. stats vector containing performance measures calibration. cl.level confidence level used. Calibration contains calibration intercept slope, together confidence intervals. Cindex value c-statistic, together confidence interval. warningMessages , warning messages printed running function. CalibrationCurves coordinates plotting calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calibration performance: ggplot version — valProbggplot","text":"using predicted probabilities uninformative model (.e. equal probabilities observations), model predictive value.  Consequently, applicable, value performance measure corresponds worst possible theoretical value. ECI, example, equals 1 (Edlinger et al., 2022).","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calibration performance: ggplot version — valProbggplot","text":"order make use (functions) package auRoc, user needs install JAGS. However, since package uses auc.nonpara.mw function depend use JAGS, therefore copied code slightly adjusted method=\"pepe\".","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibration performance: ggplot version — valProbggplot","text":"Edlinger, M, van Smeden, M, Alber, HF, Wanitschek, M, Van Calster, B. (2022). Risk prediction models discrete ordinal outcomes: Calibration impact proportional odds assumption. Statistics Medicine, 41( 8), pp. 1334– 1360 Qin, G., & Hotilovac, L. (2008). Comparison non-parametric confidence intervals area ROC curve continuous-scale diagnostic test. Statistical Methods Medical Research, 17(2), pp. 207-21 Steyerberg, E.W., Van Calster, B., Pencina, M.J. (2011). Performance measures prediction models markers : evaluation predictions classifications. Revista Espanola de Cardiologia, 64(9), pp. 788-794 Van Calster, B., Nieboer, D., Vergouwe, Y., De Cock, B., Pencina M., Steyerberg E.W. (2016). calibration hierarchy risk models defined: utopia empirical data. Journal Clinical Epidemiology, 74, pp. 167-176 Van Hoorde, K., Van Huffel, S., Timmerman, D., Bourne, T., Van Calster, B. (2015). spline-based tool assess visualize calibration multiclass risk predictions. Journal Biomedical Informatics, 54, pp. 283-93","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbggplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibration performance: ggplot version — valProbggplot","text":"","code":"# Load package library(CalibrationCurves) set.seed(1783)  # Simulate training data X      = replicate(4, rnorm(5e2)) p0true = binomial()$linkinv(cbind(1, X) %*% c(0.1, 0.5, 1.2, -0.75, 0.8)) y      = rbinom(5e2, 1, p0true) Df     = data.frame(y, X)  # Fit logistic model FitLog = lrm(y ~ ., Df)  # Simulate validation data Xval   = replicate(4, rnorm(5e2)) p0true = binomial()$linkinv(cbind(1, Xval) %*% c(0.1, 0.5, 1.2, -0.75, 0.8)) yval   = rbinom(5e2, 1, p0true) Pred   = binomial()$linkinv(cbind(1, Xval) %*% coef(FitLog))  # Default calibration plot valProbggplot(Pred, yval)  #> Call: #> valProbggplot(p = Pred, y = yval) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083   # Adding logistic calibration curves and other additional features valProbggplot(Pred, yval, CL.smooth = TRUE, logistic.cal = TRUE, lty.log = 2,  col.log = \"red\", lwd.log = 1.5)  #> Call: #> valProbggplot(p = Pred, y = yval, CL.smooth = TRUE, logistic.cal = TRUE,  #>     lty.log = 2, col.log = \"red\", lwd.log = 1.5) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083   valProbggplot(Pred, yval, CL.smooth = TRUE, logistic.cal = TRUE, lty.log = 9, col.log = \"red\", lwd.log = 1.5, col.ideal = colors()[10], lwd.ideal = 0.5)  #> Call: #> valProbggplot(p = Pred, y = yval, CL.smooth = TRUE, logistic.cal = TRUE,  #>     lty.log = 9, col.log = \"red\", lwd.log = 1.5, col.ideal = colors()[10],  #>     lwd.ideal = 0.5) #>  #> A 95% confidence interval is given for the calibration intercept, calibration slope and c-statistic.  #>  #>          Dxy      C (ROC)           R2            D     D:Chi-sq          D:p  #>   0.60048690   0.80024345   0.35491505   0.30737619 154.68809420   0.00000000  #>            U     U:Chi-sq          U:p            Q        Brier    Intercept  #>   0.01384033   8.92016298   0.01156142   0.29353586   0.18549917   0.18828469  #>        Slope         Emax Brier scaled         Eavg          ECI  #>   0.79397043   0.08026282   0.25724275   0.05093656   0.37394083"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"Plot calibration curve Cox Proportional Hazards model","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"","code":"valProbSurvival(   fit,   valdata,   alpha = 0.05,   timeHorizon = 5,   nk = 3,   plotCal = c(\"none\", \"base\", \"ggplot\"),   addCox = FALSE,   addRCS = TRUE,   CL.cox = c(\"fill\", \"line\"),   CL.rcs = c(\"fill\", \"line\"),   xlab = \"Predicted probability\",   ylab = \"Observed proportion\",   xlim = c(-0.02, 1),   ylim = c(-0.15, 1),   lty.ideal = 1,   col.ideal = \"red\",   lwd.ideal = 1,   lty.cox = 1,   col.cox = \"grey\",   lwd.cox = 1,   fill.cox = \"lightgrey\",   lty.rcs = 1,   col.rcs = \"black\",   lwd.rcs = 1,   fill.rcs = rgb(177, 177, 177, 177, maxColorValue = 255),   riskdist = \"predicted\",   d0lab = \"0\",   d1lab = \"1\",   size.d01 = 5,   dist.label = 0.01,   line.bins = -0.05,   dist.label2 = 0.04,   length.seg = 0.85,   legendloc = c(0.5, 0.27) )"},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"fit model fit, type coxph valdata validation data set alpha significance level timeHorizon time point predictions evaluated nk number knots, restricted cubic splines fit plotCal indicates calibration curve plotted. plotCal = \"none\" plots calibration curve, plotCal = \"base\" plots calibration curve using base R (see plot) plotCal = \"ggplot\" creates plot using ggplot addCox logical, indicates Cox's estimated calibration curve added plot addRCS logical, indicates restricted cubic splines' (RCS) estimated calibration curve added plot CL.cox \"fill\" shows pointwise 95% confidence limits Cox calibration curve gray area lower upper limits \"line\" shows confidence limits dotted line CL.rcs \"fill\" shows pointwise 95% confidence limits RCS calibration curve gray area lower upper limits \"line\" shows confidence limits dotted line xlab x-axis label, default \"Predicted Probability\". ylab y-axis label, default \"Observed proportion\". xlim, ylim numeric vectors length 2, giving x y coordinates ranges (see plot.window) lty.ideal linetype ideal line. Default 1. col.ideal controls color ideal line plot. Default \"red\". lwd.ideal controls line width ideal line plot. Default 1. lty.cox addCox = TRUE, linetype Cox calibration curve col.cox addCox = TRUE, color Cox calibration curve lwd.cox addCox = TRUE, linewidth Cox calibration curve fill.cox addCox = TRUE CL.cox = \"fill\", fill Cox calibration curve lty.rcs addRCS = TRUE, linetype RCS calibration curve col.rcs addRCS = TRUE, color RCS calibration curve lwd.rcs addRCS = TRUE, linewidth RCS calibration curve fill.rcs addRCS = TRUE CL.rcs = \"fill\", fill RCS calibration curve riskdist Use \"calibrated\" plot relative frequency distribution calibrated probabilities dividing 101 bins lim[1] lim[2]. Set \"predicted\" (default rms 4.5-1) use raw assigned risk, FALSE omit risk distribution. Values scaled highest bar 0.15*(lim[2]-lim[1]). d0lab, d1lab controls labels events non-events (.e. outcome y) histograms. Defaults d1lab=\"1\" events d0lab=\"0\" non-events. size.d01 controls size labels events non-events. Default 5 value multiplied 0.25 plotCal = \"base\". dist.label controls horizontal position labels events non-events. Default 0.04. line.bins controls horizontal (y-axis) position histograms. Default -0.05. dist.label2 controls vertical distance labels events non-events. Default 0.03. length.seg controls length histogram lines. Default 1. legendloc pl=TRUE, list components x,y vector c(x,y) bottom right corner legend curves points. Default c(.50, .27) scaled lim. Use locator(1) use mouse, FALSE suppress legend.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"object type SurvivalCalibrationCurves following slots: call matched call. stats list containing performance measures calibration. alpha significance level used. Calibration contains estimated calibration slope, together confidence intervals. CalibrationCurves coordinates plotting calibration curves.","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"van Geloven N, Giardiello D, Bonneville E F, Teece L, Ramspek C L, van Smeden M et al. (2022). Validation prediction models presence competing risks: guide modern methods. BMJ, 377:e069249, doi:10.1136/bmj-2021-069249","code":""},{"path":"https://bavodc.github.io/CalibrationCurves/reference/valProbSurvival.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a calibration curve for a Cox Proportional Hazards model — valProbSurvival","text":"","code":"if (FALSE) { # \\dontrun{ library(CalibrationCurves) data(trainDataSurvival) data(testDataSurvival) sFit = coxph(Surv(ryear, rfs) ~ csize + cnode + grade3, data = trainDataSurvival,  x = TRUE, y = TRUE) calPerf = valProbSurvival(sFit, gbsg5, plotCal = \"base\", nk = 5) } # }"}]
